{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ee65b8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This section installs and imports packages for use in this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab86b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary installations (more may be needed depending on your system)\n",
    "pip install stanza\n",
    "stanza.download('en')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "from random import randint\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import math\n",
    "from scipy import stats\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df7879",
   "metadata": {},
   "source": [
    "## SWN modifications\n",
    "The section below is used to investigate SWN and determine what kind of modifications may need to be made to it. Note that nothing in this section implements any actual changes: this must be done within the SWN txt file itself and/or as an 'intercept layer' (see the 'Rule-based system' section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the Stanza pipeline\n",
    "nlp = stanza.Pipeline(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some useful swn/wn functionality:\n",
    "list(swn.senti_synsets('heavy')) #lists all synset entries for a word\n",
    "\n",
    "print(wn.synset('ferocity.n.01').definition()) #shows the definition for a specific synset entry\n",
    "\n",
    "print(swn.senti_synset('breakdown.n.03')) #shows the pos/neg scores of a specific synset entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661861c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a text called \"review\" and prints info about it\n",
    "review = nlp(\"This is a review example. It has two sentences.\")\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"ROOT\"}\\tdeprel: {word.deprel}\\tpos: {word.pos}' for sent in review.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2114749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to check how many definitions a word has in swn.\n",
    "#returns a list of those with just one definition, and a list of those with multiple definitions\n",
    "#input should be a list of words you want to (potentially) modify\n",
    "\n",
    "def number_of_defs(words_to_modify):\n",
    "    single_def_words = []\n",
    "    multi_def_words = []\n",
    "    for word in words_to_modify:\n",
    "        print(\"The word\",word,\"has this number of definitions in swn:\",len(list(swn.senti_synsets(word))))\n",
    "        if len(list(swn.senti_synsets(word))) == 1:\n",
    "            single_def_words.append(word)\n",
    "        else:\n",
    "            multi_def_words.append(word)\n",
    "    print(\"The following words have only a single definition:\",single_def_words)\n",
    "    return single_def_words, multi_def_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1441c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to see which swn definition is most commonly used\n",
    "#input should be a list of sample sentences, the word you want to check, and what type that word is\n",
    "#word_type should match the types used in swn: 'n' (noun), 'r' (adverb), 'v' (verb), 'a' (adjective), 's' (adjective satellite)\n",
    "#note that adjectives have two possible types, 's' or 'a', see here: https://wordnet.princeton.edu/documentation/wndb5wn\n",
    "\n",
    "def which_def(review_texts, word, word_type):\n",
    "    results = []\n",
    "    for entry in review_texts:\n",
    "        pipelined_text = nlp(entry)\n",
    "        for sentence in pipelined_text.sentences:\n",
    "            current_sentence = []\n",
    "            for token in sentence.tokens:\n",
    "                current_sentence.append(token.text)\n",
    "            results.append(nltk.wsd.lesk(current_sentence, word, word_type))\n",
    "    c = Counter(results)\n",
    "    print(\"The word is:\",word,\"\\nThe most common result for that word was:\",c.most_common(1),\"\\nTotal results for this word were:\",results,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample useage: we want to disambiguate the word 'brutality' as a noun, so we create some fake sentences using the word\n",
    "\n",
    "brutality_snippets = [\"The brutality of this music is great.\", \"There's so much brutality in this album.\", \"Brutality, that's the dominant force here.\", \"The music is brimming over with brutality.\", \"What this album really needs is more brutality.\"]\n",
    "which_def(brutality_snippets, 'brutality', 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ae276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to search through a series of texts and find unrecognised words\n",
    "#the parser defines these as nouns by default, and they will be returned if they also have no definition in SWN\n",
    "#these can then be examined to look for new sentiment terms\n",
    "\n",
    "def new_words(reviews, existing_set = [], to_skip = []):\n",
    "    new_words = []\n",
    "    index = -1\n",
    "    for review in reviews:\n",
    "        index += 1\n",
    "        print(\"Review index number:\",index)\n",
    "        if index in to_skip:\n",
    "            continue\n",
    "        doc = nlp(review)\n",
    "        for sentence in doc.sentences:\n",
    "            for word in sentence.words:\n",
    "                if word.pos == \"NOUN\" and len(list(swn.senti_synsets(word.text))) < 1:\n",
    "                    if word.text not in new_words and word.text not in existing_set:\n",
    "                        print(\"New word being added:\",word.text)\n",
    "                        new_words.append(word.text)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b442f23",
   "metadata": {},
   "source": [
    "## Creation of corpus\n",
    "In this section we scrape websites to build the corpus. Note that the real details have been replaced with fictional examples here, and this section will need to be customised based on the sites you wish to scrape and the data you need from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function that can be used to scrape a given URL.\n",
    "#can be run in print mode (where it prints the results) or return mode, where it returns them\n",
    "\n",
    "def scrape_check(url, output=\"print\"):\n",
    "    page = urlopen(url)\n",
    "    html_bytes = page.read()\n",
    "    html = html_bytes.decode(\"utf-8\")\n",
    "    if output == \"print\":\n",
    "        print(html)\n",
    "    elif output == \"return\":\n",
    "        return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a dictionary to use for fixing incorrectly encoded characters\n",
    "#note that this is not a comprehensive list, but one based on my own scraping efforts\n",
    "#new characters may need to be added, or existing ones altered, depending on your setup and corpus\n",
    "\n",
    "fixdict = {\n",
    "    '&Auml;': 'Ä',\n",
    "    '&auml;': 'ä',\n",
    "    '&Euml;': 'Ë',\n",
    "    '&euml;': 'ë',\n",
    "    '&Iuml;': 'Ï',\n",
    "    '&iuml;': 'ï',\n",
    "    '&Ouml;': 'Ö',\n",
    "    '&ouml;': 'ö',\n",
    "    '&Uuml;': 'Ü',\n",
    "    '&uuml;': 'ü',\n",
    "    '&Yuml;': 'Ÿ',\n",
    "    '&yuml;': 'ÿ',\n",
    "    '&Aacute;': 'Á',\n",
    "    '&aacute;': 'á',\n",
    "    '&Cacute;': 'Ć',\n",
    "    '&cacute;': 'ć',\n",
    "    '&Eacute;': 'É',\n",
    "    '&eacute;': 'é',\n",
    "    '&Iacute;': 'Í',\n",
    "    '&iacute;': 'í',\n",
    "    '&Nacute;': 'Ń',\n",
    "    '&nacute;': 'ń',\n",
    "    '&Oacute;': 'Ó',\n",
    "    '&oacute;': 'ó',\n",
    "    '&Sacute;': 'Ś',\n",
    "    '&sacute;': 'ś',\n",
    "    '&Uacute;': 'Ú',\n",
    "    '&uacute;': 'ú',\n",
    "    '&Yacute;': 'Ý',\n",
    "    '&yacute;': 'ý',\n",
    "    '&Acirc;': 'Â',\n",
    "    '&acirc;': 'â',\n",
    "    '&Ecirc;': 'Ê',\n",
    "    '&ecirc;': 'ê',\n",
    "    '&Icirc;': 'Î',\n",
    "    '&icirc;': 'î',\n",
    "    '&Ocirc;': 'Ô',\n",
    "    '&ocirc;': 'ô',\n",
    "    '&Ucirc;': 'Û',\n",
    "    '&ucirc;': 'û',\n",
    "    '&AElig;': 'Æ',\n",
    "    '&aelig;': 'æ',\n",
    "    '&Ccaron;': 'Č',\n",
    "    '&ccaron;': 'č',\n",
    "    '&Agrave;': 'À',\n",
    "    '&agrave;': 'à',\n",
    "    '&Egrave;': 'È',\n",
    "    '&egrave;': 'è',\n",
    "    '&Ograve;': 'Ò',\n",
    "    '&ograve;': 'ò',\n",
    "    '&Ugrave;': 'Ù',\n",
    "    '&ugrave;': 'ù',\n",
    "    '&Aring;': 'Å',\n",
    "    '&aring;': 'å',\n",
    "    '&Oslash;': 'Ø',\n",
    "    '&oslash;': 'ø',\n",
    "    '&eth;': 'ð',\n",
    "    '&ntilde;': 'ñ',\n",
    "    '&atilde;': 'ã',\n",
    "    '&oelig;': 'œ',\n",
    "    '&ccedil;': 'ç',\n",
    "    '&THORN;': 'Þ',\n",
    "    '&amp;': '&',\n",
    "    '&quot;': '\"',\n",
    "    '&quot': '\"',\n",
    "    '&tilde;': '~',\n",
    "    '&lt;': '<',\n",
    "    '&gt;': '>',\n",
    "    '&ldquo;': '\"',\n",
    "    '&rdquo;': '\"',\n",
    "    '&mdash;': '-',\n",
    "    '&ndash;': '-',\n",
    "    '&hellip;': '...',\n",
    "    '<sup>2</sup>': '^2',\n",
    "    '<sup>3</sup>': '^3',\n",
    "    '<sup>23</sup>': '^23',\n",
    "    '&#1048;': 'И',\n",
    "    '&#1103;': 'я',\n",
    "    '&#1084;': 'м',\n",
    "    '&#1085;': 'н',\n",
    "    '&#1077;': 'е',\n",
    "    '&#1051;': 'Л',\n",
    "    '&#1075;': 'г',\n",
    "    '&#1080;': 'и',\n",
    "    '&#1086;': 'о',\n",
    "    '&#363;': 'ū',\n",
    "    '&#257;': 'ā',\n",
    "    '&#269;': 'č',\n",
    "    '&#283;': 'ě',\n",
    "    '&#378;': 'ź',\n",
    "    '&#263;': 'ć',\n",
    "    '&#279;': 'ė',\n",
    "    '&#0322;': 'ł',\n",
    "    '&#335;': 'ŏ',\n",
    "    '&#240;': 'ð',\n",
    "    '&#382;': 'ž',\n",
    "    '&#337;': 'ő',\n",
    "    '&#328;': 'ň',\n",
    "    '&#039;': \"'\",\n",
    "    '&lsquo;': \"'\",\n",
    "    '&rsquo;': \"'\",\n",
    "    '&nbsp;': '',\n",
    "    '&thorn;': 'þ',\n",
    "    '&macr;': '¯',\n",
    "    '&#1079;': 'з',\n",
    "    '&#1061;': 'Х',\n",
    "    '&#1072;': 'a',\n",
    "    '&#1089;': 'c',\n",
    "    '&#1044;': 'Д',\n",
    "    '&#1088;': 'р',\n",
    "    '&#1074;': 'в',\n",
    "    '&#1093;': 'х',\n",
    "    '&#1042;': 'В',\n",
    "    '&#51665;': '집',\n",
    "    '&Epsilon;': 'Ε',\n",
    "    '&tau;': 'τ',\n",
    "    '&epsilon;': 'ε',\n",
    "    '&rho;': 'ρ',\n",
    "    '&phi;': 'φ',\n",
    "    '&omega;': 'ω',\n",
    "    '&sigmaf;': 'ς',\n",
    "    '&Kappa;': 'Κ',\n",
    "    '&Tau;': 'Τ',\n",
    "    '&omicron;': 'o',\n",
    "    '&nu;': 'ν',\n",
    "    '&Delta;': 'Δ',\n",
    "    '&alpha;': 'α',\n",
    "    '&mu;': 'μ',\n",
    "    '&Mu;': 'M',\n",
    "    '&upsilon;': 'υ',\n",
    "    '&#22116;': '噤',\n",
    "    '&#22818;': '夢',\n",
    "    '&#947;': 'γ',\n",
    "    '&sup2;': '^2',\n",
    "    '&sup3;': '^3',\n",
    "    '&#23798;': '島',\n",
    "    '&#23996;': '嶼',\n",
    "    '&#31070;': '神',\n",
    "    '&#35441;': '話',\n",
    "    '&#491;': 'ǫ',\n",
    "    '&#259;': 'ă',\n",
    "    '&#966;': 'φ',\n",
    "    '&#351;': 'ş',\n",
    "    'O&#776;': 'Ö',\n",
    "    '&#1082;': 'к',\n",
    "    '&#1102;': 'ю',\n",
    "    '&#1095;': 'ч',\n",
    "    '&#19968;': '一',\n",
    "    '&#26399;': '期',\n",
    "    '&#20250;': '会',\n",
    "    '&#268;': 'Č',\n",
    "    '&#937;': 'Ω',\n",
    "    '&#1090;': 'т',\n",
    "    '&#322;': 'ł',\n",
    "    '&#1064;': 'Ш',\n",
    "    '&#1081;': 'й',\n",
    "    '&#921;': 'Ι',\n",
    "    '&#963;': 'σ',\n",
    "    '&#972;': 'ό',\n",
    "    '&#952;': 'θ',\n",
    "    '&#949;': 'ε',\n",
    "    '&#959;': 'ο',\n",
    "    '&#962;': 'ς',\n",
    "    '&#1110;': 'i',\n",
    "    '&#1054;': 'O',\n",
    "    '&#1050;': 'К',\n",
    "    '&#1056;': 'Р',\n",
    "    '&#1101;': 'э',\n",
    "    '&#1087;': 'п',\n",
    "    '&#1073;': 'б',\n",
    "    '&#1083;': 'л',\n",
    "    '&#1105;': 'ё',\n",
    "    '&#1076;': 'д',\n",
    "    '&#1047;': 'З',\n",
    "    '&#1099;': 'ы',\n",
    "    '&#1053;': 'H',\n",
    "    '&#1055;': 'П',\n",
    "    '&#1097;': 'щ',\n",
    "    '&#1091;': 'y',\n",
    "    '&#1096;': 'ш',\n",
    "    '&#1043;': 'Г',\n",
    "    '&#1078;': 'ж',\n",
    "    '&#1057;': 'C',\n",
    "    '&#1063;': 'Ч',\n",
    "    '&#1098;': 'ъ',\n",
    "    '&#1058;': 'T',\n",
    "    '&#1052;': 'M',\n",
    "    '&#1100;': 'ь',\n",
    "    '&#8203;': '',\n",
    "    '&#133;': '...',\n",
    "    '&#353;': 'š',\n",
    "    '&#380;': 'ż',\n",
    "    '&#183;': '·',\n",
    "    '&#345;': 'ř',\n",
    "    '&#357;': 'ť',\n",
    "    '&#352;': 'Š',\n",
    "    '&#277;': 'ĕ',\n",
    "    '&#1059;': 'У',\n",
    "    '&#1086;': 'о',\n",
    "    '&#154;': 'š',\n",
    "    '&#X10c;': 'Č',\n",
    "    '&#X10C;': 'Č',\n",
    "    '&#X107;': 'ć',\n",
    "    '&#92;': '\\\\\\\\',\n",
    "    '&#39740;': '鬼',\n",
    "    '&#7717;': 'ḥ',\n",
    "    '&#73773;': '𒀭',\n",
    "    '&#7789;': 'ṭ',\n",
    "    '&#12295;': '〇',\n",
    "    '&#7809;': 'ẁ',\n",
    "    '&#8734;': '∞',\n",
    "    '&#2357;': 'वा',\n",
    "    '&#2366;': 'घ',\n",
    "    '&#2328;': 'न',\n",
    "    '&#2344;': 'ख',\n",
    "    '&#2326;': '',\n",
    "    '&#277;': 'ĕ',\n",
    "    '&#301;': 'ĭ',\n",
    "    '&#537;': 'ș',\n",
    "    '&#917;': 'Ε',\n",
    "    '&#964;': 'τ',\n",
    "    '&#961;': 'ρ',\n",
    "    'ο&#769;': 'ό',\n",
    "    '&#969;': 'ω',\n",
    "    '&#913;': 'Α',\n",
    "    '&#945;': 'α',\n",
    "    '&#960;': 'π',\n",
    "    '&#9839;': '#',\n",
    "    '&#8734;': '∞',\n",
    "    '&#272;': 'Đ',\n",
    "    '&#273;': 'đ',\n",
    "    '&#9578;': '╪',\n",
    "    '&#650;': 'ʊ',\n",
    "    '&#26286;': '暮',\n",
    "    '&#23665;': '山',\n",
    "    '&#33337;': '船',\n",
    "    '&#24433;': '影',\n",
    "    '&#23396;': '孤',\n",
    "    '&#29128;': '燈',\n",
    "    '&#24494;': '微',\n",
    "    '&#38593;': '雁',\n",
    "    '&#32027;': '紛',\n",
    "    '&#39131;': '飛',\n",
    "    '&#28784;': '灰',\n",
    "    '&#26376;': '月',\n",
    "    '&#28472;': '漸',\n",
    "    '&#26126;': '明',\n",
    "    '&#24565;': '念',\n",
    "    '&#20234;': '伊',\n",
    "    '&#20154;': '人',\n",
    "    '&#29694;': '現',\n",
    "    '&#35937;': '象',\n",
    "    '&#40441;': '鷹',\n",
    "    '&#34892;': '行',\n",
    "    '&#21109;': '創',\n",
    "    '&#19990;': '世',\n",
    "    '&#27946;': '洪',\n",
    "    '&#27700;': '水',\n",
    "    '&#24040;': '巨',\n",
    "    '&#39250;': '饒',\n",
    "    '&#20126;': '亞',\n",
    "    '&#21746;': '哲',\n",
    "    '&#923;': 'Λ',\n",
    "    '&#942;': 'ή',\n",
    "    '&#951;': 'η',\n",
    "    '&#920;': 'Θ',\n",
    "    '&#954;': 'κ',\n",
    "    '&#946;': 'β',\n",
    "    '&#943;': 'ί',\n",
    "    '&#291;': 'ģ',\n",
    "    '&#916;': 'Δ',\n",
    "    '&#948;': 'δ',\n",
    "    '&#943;': 'ί',\n",
    "    '&#940;': 'ά',\n",
    "    '&#7941;': 'ἅ',\n",
    "    '&#7940;': 'ἄ',\n",
    "    '&#8032;': 'ὠ',\n",
    "    '&#955;': 'λ',\n",
    "    '&#953;': 'ι',\n",
    "    '&#539;': 'ț',\n",
    "    '&#1041;': 'Б',\n",
    "    '&#1237;': 'ӕ',\n",
    "    '&#1040;': 'А',\n",
    "    '&#928;': 'Π',\n",
    "    '&#905;': 'Ή',\n",
    "    '&#924;': 'Μ',\n",
    "    '&#931;': 'Σ',\n",
    "    '&#957;': 'ν',\n",
    "    '&#927;': 'Ο',\n",
    "    '&#922;': 'Κ',\n",
    "    '&#965;': 'υ',\n",
    "    '&#974;': 'ώ',\n",
    "    '&#941;': 'έ',\n",
    "    '&#956;': 'μ',\n",
    "    '&#967;': 'χ',\n",
    "    '&#932;': 'Τ',\n",
    "    '&#324;': 'ń',\n",
    "    '&#350;': 'Ş',\n",
    "    '&#973;': 'ύ',\n",
    "    '&#300;': 'Ĭ',\n",
    "    '&#355;': 'ţ',\n",
    "    '&#958;': 'ξ',\n",
    "    '&#950;': 'ζ',\n",
    "    '&#596;': 'ɔ',\n",
    "    '&#720;': 'ː',\n",
    "    '&#536;': 'Ș',\n",
    "    '&#1046;': 'Ж',\n",
    "    '&#1094;': 'ц',\n",
    "    '&#1071;': 'Я',\n",
    "    '&#369;': 'ű',\n",
    "    '&#769;': 'í',\n",
    "    '&#8206;': '',\n",
    "    '&#8195;': '',\n",
    "    '&#919;': 'Η',\n",
    "    '&#914;': 'Β',\n",
    "    '&#902;': 'Ά',\n",
    "    '&#968;': 'ψ',\n",
    "    '&#321;': 'Ł',\n",
    "    '&#1092;': 'ф',\n",
    "    '&#5573;': 'ᗅ',\n",
    "    '&#5626;': 'ᗺ',\n",
    "    '&#5623;': 'ᗷ',\n",
    "    '&#354;': 'Ţ',\n",
    "    'p&#822;': '',\n",
    "    'e&#822;': '',\n",
    "    'w&#822;': '',\n",
    "    'i&#822;': '',\n",
    "    'l&#822;': '',\n",
    "    ' &#822;': '',\n",
    "    'd&#822;': '',\n",
    "    'f&#822;': '',\n",
    "    'n&#822;': '',\n",
    "    't&#822;': '',\n",
    "    'y&#822;': '',\n",
    "    'b&#822;': '',\n",
    "    'a&#822;': '',\n",
    "    'c&#822;': '',\n",
    "    'k&#822;': '',\n",
    "    's&#822;': '',\n",
    "    'o&#822;': '',\n",
    "    'm&#822;': '',\n",
    "    'h&#822;': '',\n",
    "    'u&#822;': '',\n",
    "    'g&#822;': '',\n",
    "    '&#9834;': '♪',\n",
    "    '&#9835;': '♫',\n",
    "    '&#9745;': '☑',\n",
    "    '&#8213;': '-',\n",
    "    '&#1092;': 'к',\n",
    "    '&#1062;': 'Ц',\n",
    "    '&#1030;': 'І',\n",
    "    '&#8220;': '\"',\n",
    "    '&#8221;': '\"',\n",
    "    '&#8221': '',\n",
    "    '&#305;': 'ı',\n",
    "    '&#233;': 'é',\n",
    "    '&#248;': 'ø',\n",
    "    '&#228;': 'ä',\n",
    "    '&#244;': 'ô',\n",
    "    '&#232;': 'è',\n",
    "    '&#231;': 'ç',\n",
    "    '&#245;': 'õ',\n",
    "    '&#227;': 'ã',\n",
    "    '&#229;': 'å',\n",
    "    '&#224;': 'à',\n",
    "    '&#250;': 'ú',\n",
    "    '&#252;': 'ü',\n",
    "    '&#8211;': '-',\n",
    "    '&#8216;': \"'\",\n",
    "    '&#8217;': \"'\",\n",
    "    '&#225;': 'á',\n",
    "    '&#261;': 'ą',\n",
    "    '&#601;': 'ə',\n",
    "    '&#712;': \"'\",\n",
    "    '&#246;': 'ö',\n",
    "    '&#238;': 'î',\n",
    "    '&#235;': 'Ë',\n",
    "    '&#196;': 'Ä',\n",
    "    '&#246': 'ö',\n",
    "    '&#237;': 'í',\n",
    "    '&#347;': 'ś',\n",
    "    '&#8230;': '...',\n",
    "    '&#12484;': 'ツ',\n",
    "    '&#39072;': '颠',\n",
    "    '&#35206;': '覆',\n",
    "    '&#10003;': '✓',\n",
    "    '&#8197;': ' ',\n",
    "    '&#147;': '\"',\n",
    "    '&#148;': '\"',\n",
    "    '&#145;': \"'\",\n",
    "    '&#150;': \"-\",\n",
    "    '&#149;': \"•\",\n",
    "    '&#508;': \"Ǽ\",\n",
    "    '&#506;': \"Ǻ\",\n",
    "    '&#935;': \"Χ\",\n",
    "    '&#915;': \"Γ\",\n",
    "    '&#367;': \"ů\",\n",
    "    '&#934;': \"Φ\",\n",
    "    '&#926;': \"Ξ\",\n",
    "    '&#1605;': '', \n",
    "    '&#1576;': '',\n",
    "    '&#1610;': '',\n",
    "    '&#1583;': '',\n",
    "    '&#1575;': '',\n",
    "    '&#1604;': '',\n",
    "    '&#1583;': '',\n",
    "    '&#1617;': '',\n",
    "    '&#1606;': '',\n",
    "    '&#61521;': '',\n",
    "    '&#61484;': '',\n",
    "    '&#61472;': '',\n",
    "    '&#333;': 'ō',\n",
    "    '&#1108;': 'є',\n",
    "    '&#1045;': 'E',\n",
    "    '&#379;': 'Ż',\n",
    "    '&#908;': 'Ό',\n",
    "    '&#38291;': '間',\n",
    "    '&#332;': 'Ō',\n",
    "    '&#493;': 'Ǭ',\n",
    "    '&#246;': 'ö',\n",
    "    '&#x65e0;': '无',\n",
    "    '&#x540d;': '名',\n",
    "    '&#x645;': '',\n",
    "    '&#x6CC;': '',\n",
    "    '&#x631;': '',\n",
    "    '&#x627;': '',\n",
    "    '&#x62B;': '',\n",
    "    '&#903;': '·',\n",
    "    '&#216;': 'Ø',\n",
    "    '&#226;': 'â',\n",
    "    '&#8208;': '-',\n",
    "    '&#374;': 'Ŷ',\n",
    "    '&#198;': 'Æ',\n",
    "    '&Lcy;': 'Л',\n",
    "    '&iecy;': 'е',\n",
    "    '&gcy;': 'г',\n",
    "    '&iecy;': 'е',\n",
    "    '&ncy;': 'н',\n",
    "    '&dcy;': 'д',\n",
    "    '&acy;': 'а',\n",
    "    '&#x20;': ' ',\n",
    "    '&Ocy;': 'О',\n",
    "    '&#344;': 'Ř',\n",
    "    '&#26885;': '椅',\n",
    "    '&#23376;': '子',\n",
    "    '&#1578;': '',\n",
    "    '&#1581;': '',\n",
    "    '&#1602;': '',\n",
    "    '&#32;': '',\n",
    "    '&#1585;': '',\n",
    "    '&#1587;': '',\n",
    "    '&#1489;': 'ב',\n",
    "    '&#929;': 'P',\n",
    "    '&#255;': 'Ÿ',\n",
    "    '&#78;': 'N',\n",
    "    '&chi;': 'χ',\n",
    "    '&iota;': 'ι',\n",
    "    '&pi;': 'π',\n",
    "    '&lambda;': 'λ',\n",
    "    '&eta;': 'η',\n",
    "    '&sigma;': 'σ',\n",
    "    '&#8237;': '',\n",
    "    '&#8236;': '',\n",
    "    '&#146;': \"'\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions used to clean scraped text.\n",
    "#character_code_cleanup fixes based on the fixdict dictionary\n",
    "#clean_text removes anything within angled brackets (which will typically be formatting markers)\n",
    "\n",
    "def character_code_cleanup(column):\n",
    "    for code, character in fixdict.items():\n",
    "        column = [re.sub(code, character, entry) for entry in column]\n",
    "    return column\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub('<[^>]+>', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d127cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#an example of a function for scraping a particular site of reviews.\n",
    "#note that this will need to be modified depending on the site's particular layout/architecture, and what you wish to scrape\n",
    "\n",
    "def example_scrape(dir_links):\n",
    "    #lists to contain review links, texts, ratings and titles\n",
    "    review_links_list = []\n",
    "    all_review_texts = []\n",
    "    review_titles = []\n",
    "    review_ratings = []\n",
    "    \n",
    "    #markers for the start and end of each of the relevant fields\n",
    "    #each start marker should be a unique piece of text that marks where the relevant field begins on that webpage. If it is not unique, the scraping will pick up incorrect info\n",
    "    #the end marker should then mark where it ends (though it does not need to be unique, as the code will simply find the next instance of it after the start)\n",
    "    #the start/end points do not need to be perfect, but the more unnecessary text that is scraped, the more cleaning will have to be done afterwards\n",
    "    title_start_marker = 'review_title='\n",
    "    title_end_marker = '/end_title'\n",
    "    review_start_marker = 'review_body='\n",
    "    review_end_marker = '/end_body'\n",
    "    rating_start_marker = 'review_rating='\n",
    "    rating_end_marker = '/end_rating'\n",
    "    \n",
    "    #in this example, we have a website with directories of all review URL links\n",
    "    #these directories will be scraped to gather all review URLs, which can each then be scraped for the data we want\n",
    "    #in this particular case, every review URL takes the form \"http://www.example.com/reviewid=\", then a numbered ID of the review\n",
    "    #so we can scrape the directories for the numbered IDs, then paste them onto this base URL to create links to the reviews\n",
    "    #id_start_marker indicates the marker within the directory pages that precedes a numbered review ID\n",
    "    example_review_baselink = \"http://www.example.com/reviewid=\"\n",
    "    id_start_marker = '<a href=\"/reviews/id='\n",
    "    \n",
    "    #we work through each directory listed in dir_links, scraping it\n",
    "    for directory in dir_links:\n",
    "        current_page = scrape_check(directory, output=\"return\")\n",
    "        #we work through the page to find every review link listed on the page, extracting the review id and pasting it onto the baselink\n",
    "        while len(current_page) > 1:\n",
    "            try:\n",
    "                current_point_start = (current_page.index(id_start_marker)) + len(id_start_marker)\n",
    "                #note that you may need to tweak the number below to ensure that the correct piece of text is extracted and nothing more\n",
    "                current_point_end = current_point_start + 5\n",
    "                revid = current_page[current_point_start:current_point_end]\n",
    "                #the two lines below ensure that only digits (i.e. the review ID number) are extracted\n",
    "                numeric_filter = filter(str.isdigit, revid)\n",
    "                revid = \"\".join(numeric_filter)\n",
    "                link = example_review_baselink + revid\n",
    "                review_links_list.append(link)\n",
    "                current_page = current_page[current_point_end:]\n",
    "            except ValueError:\n",
    "                current_page = 'a'\n",
    "                pass\n",
    "    \n",
    "    #having gone through every directory, we now have a list of all review URLs\n",
    "    review_links_list = list(dict.fromkeys(review_links_list))\n",
    "    #now we scrape every review URL in that list, and extract the desired information from it\n",
    "    #in this example specifically, we are extracting the review title, review text and review rating, and they appear in that order on the page\n",
    "    #note that this function removes the scraped page results as it goes, so the below must go in the order the desired fields appear on the page\n",
    "    for revlink in review_links_list:\n",
    "        print(\"Current review scrape:\",revlink)\n",
    "        full_page = scrape_check(revlink, output=\"return\")\n",
    "        title_start = (full_page.index(title_start_marker)) + len(title_start_marker)\n",
    "        full_page = full_page[title_start:]\n",
    "        title_end = full_page.index(title_end_marker)\n",
    "        title_text = full_page[:title_end]\n",
    "        review_titles.append(title_text)\n",
    "        review_start = (full_page.index(review_start_marker)) + len(review_start_marker)\n",
    "        full_page = full_page[review_start:]\n",
    "        review_end = full_page.index(review_end_marker)\n",
    "        review_text = full_page[:review_end]\n",
    "        all_review_texts.append(review_text)\n",
    "        try:\n",
    "            rating_start = (full_page.index(rating_start_marker)) + len(rating_start_marker)\n",
    "            full_page = full_page[rating_start:]\n",
    "            rating_end = full_page.index(rating_end_marker)\n",
    "            review_rating = full_page[:rating_end]\n",
    "        #in some cases the review might not have a rating, and needs to be skipped\n",
    "        except ValueError:\n",
    "            review_rating = 'N/A'\n",
    "            pass\n",
    "        review_ratings.append(review_rating)\n",
    "    #then we return all scraped data\n",
    "    return all_review_texts, review_titles, review_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb32531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example lists creation\n",
    "#there are 54 directories in this example, with each directory URL ending in a number, 1-54\n",
    "#so we can simply create a list of all directory pages with the base of that URL + the number\n",
    "\n",
    "example_directory_links = []\n",
    "example_directory_baselink = 'http://www.example.com/directory='\n",
    "for i in range(1,55):\n",
    "    link = example_directory_baselink + str(i)\n",
    "    example_directory_links.append(link)\n",
    "\n",
    "#then we can scrape this collection of directory links, getting their review links, then the desired data from each of those reviews\n",
    "example_reviews, example_titles, example_ratings = example_scrape(example_directory_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b0e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup of the review column\n",
    "#again, the exact details will vary by site\n",
    "#in this case, we want to remove some characters from the start of each review text\n",
    "#then clean up the character encodings and remove any formatting markers within <> brackets\n",
    "\n",
    "example_reviews = [x[13:] for x in example_reviews]\n",
    "example_reviews = character_code_cleanup(example_reviews)\n",
    "for i, x in enumerate(example_reviews):\n",
    "    example_reviews[i] = clean_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will also clean up the titles column (though this has no formatting markers, so only the character encoding cleanup is needed)\n",
    "#this also includes removing some unnecessary text at the end of each title\n",
    "\n",
    "for i, x in enumerate(example_titles):\n",
    "    example_titles[i] = x[:-10]\n",
    "example_titles = character_code_cleanup(example_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create lists to act as columns in the final dataframe, detailing the source of scraping an index that can later act as a unique ID number for each review\n",
    "example_origin = ['example.com'] * len(example_ratings)\n",
    "example_index = []\n",
    "for i in range(0, len(example_ratings)):\n",
    "    example_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff2cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the lists can then be transformed into a pandas dataframe, then downloaded as a csv file\n",
    "d_example = {'Index': example_index, 'Source': example_origin, 'Title': example_titles, 'Rating': example_ratings, 'Review': example_reviews}\n",
    "df_example = pd.DataFrame(d_example)\n",
    "df_example.to_csv('example.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4c90e",
   "metadata": {},
   "source": [
    "## Rule-based system\n",
    "This section contains the main rule-based system of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not done earlier, we set up the Stanza pipeline\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define a full list of all explicit and implicit aspect terms, and create a dictionary structure with aspect categories as keys and all terms as values\n",
    "\n",
    "full_aspect_list = ['guitar', 'guitarist', 'riff', 'riffing', 'lead', 'solo', 'soloing', 'noodling', 'riffage', 'vocalist', 'singer', 'vocal', 'vocalisation', 'vocalising', 'vokill', 'singing', 'scream', 'screaming', 'growl', 'growling', 'roar', 'roaring', 'shriek', 'shrieking', 'wail', 'wailing', 'snarl', 'snarling', 'bark', 'barking', 'howl', 'howling', 'yell', 'yelling', 'chant', 'chanting', 'croon', 'crooning', 'voice', 'vox', 'drummer', 'percussionist', 'drum', 'drumming', 'fill', 'beat', 'percussion', 'production', 'mastering', 'master', 'mix', 'production job', 'producer', 'recording', 'breakdown', 'lyric', 'theme', 'ambience', 'mood', 'atmosphere', 'composition', 'structure', 'writing', 'songwriting', 'arrangement', 'bass', 'bassline', 'bassist', 'melody', 'harmony', 'rhythm', 'grooves', 'technicality', 'complexity', 'signature', 'skill', 'talent', 'musicianship', 'keyboard', 'synth', 'piano', 'orchestra', 'choir', 'symphonic', 'orchestration', 'sax', 'saxophone', 'tambourine', 'trumpet', 'flute', 'creativity', 'experimentation', 'variation', 'variety', 'diversity', 'humor', 'humour', 'comedy', 'memorability', 'catchiness']\n",
    "implicit_aspects = ['song', 'album', 'track', 'sound', 'band', 'artist', 'group', 'musician', 'performer', 'member', 'music']\n",
    "aspect_synonyms = {'implicit': ['song', 'album', 'track', 'sound', 'band', 'artist', 'group', 'musician', 'performer', 'member', 'music'], 'guitars': ['guitar', 'guitarist', 'riff', 'riffing', 'lead', 'solo', 'soloing', 'noodling', 'riffage'], 'vocals': ['vocalist', 'singer', 'vocal', 'vocalisation', 'vocalising', 'vokill', 'singing', 'scream', 'screaming', 'growl', 'growling', 'roar', 'roaring', 'shriek', 'shrieking', 'wail', 'wailing', 'snarl', 'snarling', 'bark', 'barking', 'howl', 'howling', 'yell', 'yelling', 'chant', 'chanting', 'croon', 'crooning', 'voice', 'vox'], 'drums': ['drummer', 'percussionist', 'drum', 'drumming', 'fill', 'beat', 'percussion'], 'production': ['production', 'mastering', 'master', 'mix', 'production job', 'producer', 'recording'], 'breakdowns': ['breakdown'], 'lyrics': ['lyric', 'theme'], 'ambience': ['ambience', 'mood', 'atmosphere'], 'writing': ['composition', 'structure', 'writing', 'songwriting', 'arrangement'], 'bass': ['bass', 'bassline', 'bassist'], 'melodies': ['melody', 'harmony', 'rhythm', 'grooves'], 'technicality': ['technicality', 'complexity', 'signature', 'skill', 'talent', 'musicianship'], 'keyboards': ['keyboard', 'synth', 'piano'], 'symphonics': ['orchestra', 'choir', 'symphonic', 'orchestration'], 'exotic instruments': ['sax', 'saxophone', 'tambourine', 'trumpet', 'flute'], 'creativity': ['creativity', 'experimentation', 'variation', 'variety', 'diversity', 'inspiration'], 'comedy': ['humor', 'humour', 'comedy'], 'catchiness': ['memorability', 'catchiness']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the intercept layer for new words and their sentiment scores.\n",
    "#this includes words completely new to SWN and those which do exist in SWN, but have a new specific meaning within this domain which is not used by SWN.\n",
    "#these new meaning entries should only be added where the new, domain-specific meaning is very dominant (i.e. if you see this word in this domain, it will almost always be used in this new sense)\n",
    "#note that hyphenated terms are merged, as the system will modify review text to convert such terms and allow them to be recognised\n",
    "\n",
    "new_sentiment_terms = {\n",
    "    \"hardrocking\": 0.375,\n",
    "    \"hardrock\": 0.375,\n",
    "    \"headbanging\": 0.375,\n",
    "    \"rock\": 0.375,\n",
    "    \"rocker\": 0.375,\n",
    "    \"masterclass\": 0.625,\n",
    "    \"masterwork\": 0.625,\n",
    "    \"brutalise\": 0.25,\n",
    "    \"brutalize\": 0.25,\n",
    "    \"brutaliser\": 0.25,\n",
    "    \"brutalizer\": 0.25,\n",
    "    \"bythenumbers\": -0.25,\n",
    "    \"runofthemill\": -0.25,\n",
    "    \"cringeworthy\": -0.75,\n",
    "    \"cringey\": -0.75,\n",
    "    \"cringy\": -0.75,\n",
    "    \"wankery\": -0.5,\n",
    "    \"thrashy\": 0.375,\n",
    "    \"catchiness\": 0.125,\n",
    "    \"headbang\": 0.375,\n",
    "    \"headbanger\": 0.375,\n",
    "    \"bang\": 0.375,\n",
    "    \"banger\": 0.375,\n",
    "    \"skipworthy\": -0.25,\n",
    "    \"forgettability\": -0.5,\n",
    "    \"rollercoaster\": 0.375,\n",
    "    \"cliché\": -0.375,\n",
    "    \"cliche\": -0.375,\n",
    "    \"trve\": -0.5,\n",
    "    \"tr00\": -0.5,\n",
    "    \"kvlt\": -0.5,\n",
    "    \"cvlt\": -0.5,\n",
    "    \"stratospheric\": 0.25,\n",
    "    \"asshat\": -0.625,\n",
    "    \"knuckledragger\": -0.375,\n",
    "    \"clubcore\": -0.5,\n",
    "    \"br00tal\": -0.5,\n",
    "    \"brootal\": -0.5,\n",
    "    \"br00tality\": -0.5,\n",
    "    \"brootality\": -0.5,\n",
    "    \"musthear\": 0.375,\n",
    "    \"mustlisten\": 0.375,\n",
    "    \"poorman's\": -0.875,\n",
    "    \"standout\": 0.375,\n",
    "    \"mallcore\": -0.5,\n",
    "    \"oversaturation\": -0.375,\n",
    "    \"oversaturated\": -0.375,\n",
    "    \"oversaturate\": -0.375,\n",
    "    \"butthurt\": -0.625,\n",
    "    \"burnout\": -0.5,\n",
    "    \"burntout\": -0.5,\n",
    "    \"fistpump\": 0.375,\n",
    "    \"hornthrow\": 0.375,\n",
    "    \"necksnap\": 0.375,\n",
    "    \"fistpumper\": 0.375,\n",
    "    \"hornthrower\": 0.375,\n",
    "    \"necksnapper\": 0.375,\n",
    "    \"mudslinging\": 0.375,\n",
    "    \"listenability\": 0.625,\n",
    "    \"awesomeness\": 0.75,\n",
    "    \"thrashiness\": 0.375,\n",
    "    \"doomster\": 0.375,\n",
    "    \"noholdsbarred\": 0.25,\n",
    "    \"snorefest\": -0.25,\n",
    "    \"snoozefest\": -0.25,\n",
    "    \"borefest\": -0.25,\n",
    "    \"musthave\": 0.375,\n",
    "    \"vomitinduce\": -0.5,\n",
    "    \"turnoff\": -0.75,\n",
    "    \"pitchperfect\": 1.0,\n",
    "    \"clunker\": -0.625,\n",
    "    \"ripoff\": -0.25,\n",
    "    \"pulserace\": 0.375,\n",
    "    \"trainwreck\": -0.375,\n",
    "    \"badass\": 0.375,\n",
    "    \"infectiousness\": 0.125,\n",
    "    \"infectious\": 0.125,\n",
    "    \"crustiness\": 0.375,\n",
    "    \"selfparody\": -0.375,\n",
    "    \"sellout\": -0.625,\n",
    "    \"soldout\": -0.625,\n",
    "    \"skullcrush\": 0.25,\n",
    "    \"skullcrushingly\": 0.25,\n",
    "    \"eargasm\": 0.375,\n",
    "    \"hardhitting\": 0.25,\n",
    "    \"hardhit\": 0.25,\n",
    "    \"earworm\": 0.125,\n",
    "    \"infectious\": 0.25,\n",
    "    \"heavy\": 0.375,\n",
    "    \"heaviness\": 0.375,\n",
    "    \"harsh\": 0,\n",
    "    \"harshness\": 0,\n",
    "    \"filler\": -0.25,\n",
    "    \"generic\": -0.375,\n",
    "    \"shredding\": 0,\n",
    "    \"shred\": 0,\n",
    "    \"killer\": 0.375,\n",
    "    \"solid\": 0.25,\n",
    "    \"distorted\": 0,\n",
    "    \"distortion\": 0,\n",
    "    \"brutal\": 0.25,\n",
    "    \"clean\": 0,\n",
    "    \"dark\": 0.25,\n",
    "    \"rubbish\": -0.375,\n",
    "    \"dirty\": 0,\n",
    "    \"filthy\": 0,\n",
    "    \"comeback\": 0,\n",
    "    \"live\": 0,\n",
    "    \"progressive\": 0,\n",
    "    \"thrash\": 0,\n",
    "    \"sludge\": 0,\n",
    "    \"industrial\": 0,\n",
    "    \"fulllength\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a list of modifier terms which will alter the sentiment score of an attached word by a certain proportion\n",
    "\n",
    "modifiers = {\n",
    "    \"less\": -1.5,\n",
    "    \"barely\": -1.5,\n",
    "    \"hardly\": -1.5,\n",
    "    \"almost\": -1.5,\n",
    "    \"nottoo\": -1.5,\n",
    "    \"notonly\": 0.5,\n",
    "    \"notjust\": 0.5,\n",
    "    \"notsimply\": 0.5,\n",
    "    \"only\": -0.5,\n",
    "    \"alittle\": -0.5,\n",
    "    \"alittlebit\": -0.5,\n",
    "    \"slightly\": -0.5,\n",
    "    \"marginally\": -0.5,\n",
    "    \"relatively\": -0.3,\n",
    "    \"mildly\": -0.3,\n",
    "    \"moderately\": -0.3,\n",
    "    \"somewhat\": -0.3,\n",
    "    \"partially\": -0.3,\n",
    "    \"abit\": -0.3,\n",
    "    \"arguably\": -0.2,\n",
    "    \"mostly\": -0.2,\n",
    "    \"mainly\": -0.2,\n",
    "    \"theleastbit\": -0.9,\n",
    "    \"tosomeextent\": -0.2,\n",
    "    \"toacertainextent\": -0.2,\n",
    "    \"sortof\": -0.3,\n",
    "    \"sorta\": -0.3,\n",
    "    \"kindof\": -0.3,\n",
    "    \"kinda\": -0.3,\n",
    "    \"fairly\": -0.2,\n",
    "    \"pretty\": -0.1,\n",
    "    \"rather\": -0.1,\n",
    "    \"immediately\": 0.1,\n",
    "    \"quite\": 0.1,\n",
    "    \"perfectly\": 0.1,\n",
    "    \"consistently\": 0.1,\n",
    "    \"really\": 0.2,\n",
    "    \"clearly\": 0.2,\n",
    "    \"obviously\": 0.2,\n",
    "    \"certainly\": 0.2,\n",
    "    \"completely\": 0.2,\n",
    "    \"definitely\": 0.2,\n",
    "    \"absolutely\": 0.2,\n",
    "    \"constantly\": 0.2,\n",
    "    \"highly\": 0.2,\n",
    "    \"very\": 0.2,\n",
    "    \"significantly\": 0.2,\n",
    "    \"noticeably\": 0.2,\n",
    "    \"distinctively\": 0.2,\n",
    "    \"frequently\": 0.2,\n",
    "    \"awfully\": 0.2,\n",
    "    \"totally\": 0.2,\n",
    "    \"largely\": 0.2,\n",
    "    \"fully\": 0.2,\n",
    "    \"extra\": 0.3,\n",
    "    \"truly\": 0.3,\n",
    "    \"especially\": 0.3,\n",
    "    \"particularly\": 0.3,\n",
    "    \"damn\": 0.3,\n",
    "    \"intensively\": 0.3,\n",
    "    \"downright\": 0.3,\n",
    "    \"entirely\": 0.3,\n",
    "    \"strongly\": 0.3,\n",
    "    \"remarkably\": 0.3,\n",
    "    \"majorly\": 0.3,\n",
    "    \"amazingly\": 0.3,\n",
    "    \"strikingly\": 0.3,\n",
    "    \"stunningly\": 0.3,\n",
    "    \"quintessentially\": 0.3,\n",
    "    \"unusually\": 0.3,\n",
    "    \"dramatically\": 0.3,\n",
    "    \"intensely\": 0.3,\n",
    "    \"extremely\": 0.4,\n",
    "    \"so\": 0.4,\n",
    "    \"incredibly\": 0.4,\n",
    "    \"terribly\": 0.4,\n",
    "    \"hugely\": 0.4,\n",
    "    \"immensely\": 0.4,\n",
    "    \"such\": 0.4,\n",
    "    \"unbelievably\": 0.4,\n",
    "    \"insanely\": 0.4,\n",
    "    \"outrageously\": 0.4,\n",
    "    \"radically\": 0.4,\n",
    "    \"blisteringly\": 0.4,\n",
    "    \"exceptionally\": 0.4,\n",
    "    \"exceedingly\": 0.4,\n",
    "    \"withoutadoubt\": 0.4,\n",
    "    \"way\": 0.4,\n",
    "    \"vastly\": 0.4,\n",
    "    \"deeply\": 0.4,\n",
    "    \"super\": 0.4,\n",
    "    \"profoundly\": 0.4,\n",
    "    \"universally\": 0.4,\n",
    "    \"abundantly\": 0.4,\n",
    "    \"infinitely\": 0.4,\n",
    "    \"exponentially\": 0.4,\n",
    "    \"enormously\": 0.4,\n",
    "    \"thoroughly\": 0.4,\n",
    "    \"passionately\": 0.4,\n",
    "    \"tremendously\": 0.4,\n",
    "    \"ridiculously\": 0.4,\n",
    "    \"obscenely\": 0.4,\n",
    "    \"wildly\": 0.4,\n",
    "    \"extraordinarily\": 0.5,\n",
    "    \"spectacularly\": 0.5,\n",
    "    \"phenomenally\": 0.5,\n",
    "    \"monumentally\": 0.5,\n",
    "    \"mind-bogglingly\": 0.5,\n",
    "    \"utterly\": 0.5,\n",
    "    \"more\": -0.5,\n",
    "    \"evenmore\": 0.5,\n",
    "    \"morethan\": 0.5,\n",
    "    \"themost\": 1.0,\n",
    "    \"utmost\": 1.0,\n",
    "    \"total\": 0.5,\n",
    "    \"monumental\": 0.5,\n",
    "    \"great\": 0.5,\n",
    "    \"greatly\": 0.5,\n",
    "    \"huge\": 0.5,\n",
    "    \"tremendous\": 0.5,\n",
    "    \"complete\": 0.4,\n",
    "    \"infinite\": 0.4,\n",
    "    \"endless\": 0.4,\n",
    "    \"absolute\": 0.5,\n",
    "    \"resounding\": 0.4,\n",
    "    \"unabashed\": 0.4,\n",
    "    \"dropdead\": 0.4,\n",
    "    \"massive\": 0.5,\n",
    "    \"collossal\": 0.5,\n",
    "    \"incredible\": 0.5,\n",
    "    \"unimagiable\": 0.5,\n",
    "    \"abject\": 0.5,\n",
    "    \"sucha\": 0.4,\n",
    "    \"suchan\": 0.4,\n",
    "    \"utter\": 0.4,\n",
    "    \"double\": 0.3,\n",
    "    \"clear\": 0.3,\n",
    "    \"clearer\": 0.2,\n",
    "    \"clearest\": 0.5,\n",
    "    \"big\": 0.3,\n",
    "    \"bigger\": 0.2,\n",
    "    \"biggest\": 0.5,\n",
    "    \"obvious\": 0.3,\n",
    "    \"serious\": 0.3,\n",
    "    \"deep\": 0.3,\n",
    "    \"deeper\": 0.2,\n",
    "    \"deepest\": 0.5,\n",
    "    \"considerable\": 0.2,\n",
    "    \"important\": 0.3,\n",
    "    \"major\": 0.2,\n",
    "    \"crucial\": 0.3,\n",
    "    \"immediate\": 0.1,\n",
    "    \"visable\": 0.1,\n",
    "    \"noticeable\": 0.1,\n",
    "    \"consistent\": 0.1,\n",
    "    \"high\": 0.2,\n",
    "    \"higher\": 0.1,\n",
    "    \"highest\": 0.5,\n",
    "    \"real\": 0.2,\n",
    "    \"true\": 0.2,\n",
    "    \"pure\": 0.2,\n",
    "    \"definite\": 0.2,\n",
    "    \"much\": 0.2,\n",
    "    \"small\": -0.3,\n",
    "    \"smaller\": -0.2,\n",
    "    \"smallest\": -0.5,\n",
    "    \"minor\": -0.3,\n",
    "    \"moderate\": -0.3,\n",
    "    \"mild\": -0.3,\n",
    "    \"slight\": -0.5,\n",
    "    \"slightest\": -0.9,\n",
    "    \"insignificant\": -0.5,\n",
    "    \"inconsequential\": -0.5,\n",
    "    \"low\": -2.0,\n",
    "    \"lowest\": -3.0,\n",
    "    \"few\": -2.0,\n",
    "    \"fewer\": -1.5,\n",
    "    \"fewest\": -3.0,\n",
    "    \"alot\": 0.3,\n",
    "    \"numerous\": 0.3,\n",
    "    \"several\": 0.2,\n",
    "    \"multiple\": 0.2,\n",
    "    \"various\": 0.2,\n",
    "    \"afew\": -0.3,\n",
    "    \"acouple\": -0.3,\n",
    "    \"acoupleof\": -0.3,\n",
    "    \"alotof\": 0.3,\n",
    "    \"lotsof\": 0.3,\n",
    "    \"atall\": -0.5,\n",
    "    \"agreatdealof\": 0.5,\n",
    "    \"awholelotof\": 0.5,\n",
    "    \"ahugeamountof\": 0.5,\n",
    "    \"hugenumbersof\": 0.5,\n",
    "    \"aheckofa\": 0.5,\n",
    "    \"ahellofa\": 0.5,\n",
    "    \"aplethoraof\": 0.5,\n",
    "    \"amultitudeof\": 0.5,\n",
    "    \"heckuva\": 0.5,\n",
    "    \"atonof\": 0.5,\n",
    "    \"tonsof\": 0.5,\n",
    "    \"abunchof\": 0.3,\n",
    "    \"bunchesof\": 0.3,\n",
    "    \"plentyof\": 0.3,\n",
    "    \"acertainamountof\": -0.2,\n",
    "    \"some\": -0.2,\n",
    "    \"alittlebitof\": -0.5,\n",
    "    \"abitof\": -0.5,\n",
    "    \"abitofa\": -0.5,\n",
    "    \"difficultto\": -1.5,\n",
    "    \"hardto\": -1.5,\n",
    "    \"toughto\": -1.5,\n",
    "    \"nowherenear\": -3.0,\n",
    "    \"notallthat\": -1.2,\n",
    "    \"notthat\": -1.5,\n",
    "    \"outof\": -2.0,\n",
    "    \"scarcely\": -1.5,\n",
    "    \"endlessly\": 0.4,\n",
    "    \"resoundingly\": 0.4,\n",
    "    \"unabashedly\": 0.4,\n",
    "    \"massively\": 0.5,\n",
    "    \"abjectly\": 0.5,\n",
    "    \"doubly\": 0.3,\n",
    "    \"seriously\": 0.3,\n",
    "    \"purely\": 0.2,\n",
    "    \"lackof\": -0.5,\n",
    "    \"toolittle\": -0.3,\n",
    "    \"toofew\": -0.3,\n",
    "    \"toomuch\": -0.3,\n",
    "    \"toomany\": -0.3,\n",
    "    \"not\": -0.5,\n",
    "    \"no\": -0.5,\n",
    "    \"noone\": -0.5,\n",
    "    \"none\": -0.5,\n",
    "    \"nobody\": -0.5,\n",
    "    \"nothing\": -0.5,\n",
    "    \"neither\": -0.5,\n",
    "    \"nor\": -0.5,\n",
    "    \"nowhere\": -0.5,\n",
    "    \"n't\": -0.5,\n",
    "    \"without\": -0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5542054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi-word or hyphenated new terms and modifier terms, which may not be recognised, are compressed by the system\n",
    "\n",
    "words_to_modify = {\n",
    "    \"hard-rocking\": \"hardrocking\",\n",
    "    \"head-banging\": \"headbanging\",\n",
    "    \"by-the-numbers\": \"bythenumbers\",\n",
    "    \"run-of-the-mill\": \"runofthemill\",\n",
    "    \"cringe-worthy\": \"cringeworthy\",\n",
    "    \"thrash-y\": \"thrashy\",\n",
    "    \"re-defining\": \"redefining\",\n",
    "    \"skip-worthy\": \"skipworthy\",\n",
    "    \"must-hear\": \"musthear\",\n",
    "    \"must-listen\": \"mustlisten\",\n",
    "    \"poor-man's\": \"poorman's\",\n",
    "    \"over-saturation\": \"oversaturation\",\n",
    "    \"over-saturated\": \"oversaturated\",\n",
    "    \"re-hash\": \"rehash\",\n",
    "    \"butt-hurt\": \"butthurt\",\n",
    "    \"burn-out\": \"burnout\",\n",
    "    \"burnt-out\": \"burntout\",\n",
    "    \"fist-pumping\": \"fistpumping\",\n",
    "    \"horn-throwing\": \"hornthrowing\",\n",
    "    \"neck-snapping\": \"necksnapping\",\n",
    "    \"fist-pumper\": \"fistpumper\",\n",
    "    \"over-saturate\": \"oversaturate\",\n",
    "    \"horn-thrower\": \"hornthrower\",\n",
    "    \"neck-snapper\": \"necksnapper\",\n",
    "    \"no-holds-barred\": \"noholdsbarred\",\n",
    "    \"snore-fest\": \"snorefest\",\n",
    "    \"snooze-fest\": \"snooze-fest\",\n",
    "    \"bore-fest\": \"borefest\",\n",
    "    \"must-have\": \"musthave\",\n",
    "    \"vomit-inducing\": \"vomitinducing\",\n",
    "    \"turn-off\": \"turnoff\",\n",
    "    \"pitch-perfect\": \"pitchperfect\",\n",
    "    \"rip-off\": \"ripoff\",\n",
    "    \"pulse-racing\": \"pulseracing\",\n",
    "    \"bad-ass\": \"badass\",\n",
    "    \"jaw-dropping\": \"jawdropping\",\n",
    "    \"jaw-droppingly\": \"jawdroppingly\",\n",
    "    \"self-parody\": \"selfparody\",\n",
    "    \"pit-fall\": \"pitfall\",\n",
    "    \"sell-out\": \"sellout\",\n",
    "    \"sold-out\": \"soldout\",\n",
    "    \"skull-crushing\": \"skullcrushing\",\n",
    "    \"skull-crshingly\": \"skullcrushingly\",\n",
    "    \"hard-hitting\": \"hardhitting\",\n",
    "    \"full-length\": \"fulllength\",\n",
    "    \"not too\": \"nottoo\",\n",
    "    \"not only\": \"notonly\",\n",
    "    \"not just\": \"notjust\",\n",
    "    \"not simply\": \"notsimply\",\n",
    "    \"a little\": \"alittle\",\n",
    "    \"a little bit\": \"alittlebit\",\n",
    "    \"a bit\": \"abit\",\n",
    "    \"the least bit\": \"theleastbit\",\n",
    "    \"to some extent\": \"tosomeextent\",\n",
    "    \"to a certain extent\": \"toacertainextent\",\n",
    "    \"sort of\": \"sortof\",\n",
    "    \"kind of\": \"kindof\",\n",
    "    \"without a doubt\": \"withoutadoubt\",\n",
    "    \"even more\": \"evenmore\",\n",
    "    \"more than\": \"morethan\",\n",
    "    \"the most\": \"themost\",\n",
    "    \"drop dead\": \"dropdead\",\n",
    "    \"such a\": \"sucha\",\n",
    "    \"such an\": \"suchan\",\n",
    "    \"a lot\": \"alot\",\n",
    "    \"a few\": \"afew\",\n",
    "    \"a couple\": \"acouple\",\n",
    "    \"a couple of\": \"acoupleof\",\n",
    "    \"a lot of\": \"alotof\",\n",
    "    \"lots of\": \"lotsof\",\n",
    "    \"at all\": \"atall\",\n",
    "    \"a great deal of\": \"agreatdealof\",\n",
    "    \"a whole lot of\": \"awholelotof\",\n",
    "    \"a huge amount of\": \"ahugeamountof\",\n",
    "    \"huge numbers of\": \"hugenumbersof\",\n",
    "    \"a heck of a\": \"aheckofa\",\n",
    "    \"a hell of a\": \"ahellofa\",\n",
    "    \"a plethora of\": \"aplethoraof\",\n",
    "    \"a multitude of\": \"amultitudeof\",\n",
    "    \"a ton of\": \"atonof\",\n",
    "    \"tons of\": \"tonsof\",\n",
    "    \"a bunch of\": \"abunchof\",\n",
    "    \"bunches of\": \"bunchesof\",\n",
    "    \"plenty of\": \"plentyof\",\n",
    "    \"a certain amount of\": \"acertainamountof\",\n",
    "    \"a little bit of\": \"alittlebitof\",\n",
    "    \"a bit of\": \"abitof\",\n",
    "    \"a bit of a\": \"abitofa\",\n",
    "    \"difficult to\": \"difficultto\",\n",
    "    \"hard to\": \"hardto\",\n",
    "    \"tough to\": \"toughto\",\n",
    "    \"nowhere near\": \"nowherenear\",\n",
    "    \"not all that\": \"notallthat\",\n",
    "    \"not that\": \"notthat\",\n",
    "    \"out of\": \"outof\",\n",
    "    \"lack of\": \"lackof\",\n",
    "    \"too little\": \"toolittle\",\n",
    "    \"too few\": \"toofew\",\n",
    "    \"too much\": \"toomuch\",\n",
    "    \"too many\": \"toomany\",\n",
    "    \"no one\": \"noone\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb296036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the full rule-based system\n",
    "\n",
    "#a function that checks for the presence of aspect terms within a sentence. Implicit only counted if a noun.\n",
    "#also checks for compound terms.\n",
    "#returns the id of all found aspect terms\n",
    "\n",
    "def aspect_detection(sent):\n",
    "    #check if a sentence contains any aspects, and return their details\n",
    "    aspects_ids = []\n",
    "    for sentence in sent.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.lemma in full_aspect_list or (word.lemma in implicit_aspects and word.upos == \"NOUN\"):\n",
    "                details = [word.id, word.lemma, word.head]\n",
    "                #if the aspect word is marked as a \"compound\", then we need to extend the search\n",
    "                if word.deprel == \"compound\":\n",
    "                    compound = word.head\n",
    "                    #so we search again, looking for the head of the compound word, then getting ITS head (and later its dependent terms)\n",
    "                    for word in sentence.words:\n",
    "                        if word.id == compound:\n",
    "                            details.append(word.head)\n",
    "                aspects_ids.append(details)\n",
    "    return aspects_ids\n",
    "\n",
    "#a function that checks the category of a given aspect term\n",
    "def aspect_category(foundword):\n",
    "    aspect = None\n",
    "    aspectlist = aspect_synonyms.items()\n",
    "    for item in aspectlist:\n",
    "        for sub_item in item[1:]:\n",
    "            if foundword in sub_item:\n",
    "                aspect = item[0]\n",
    "    return aspect\n",
    "\n",
    "#a function that finds all words in a sentence with a particular aspect (by its id) as their head\n",
    "#returns the id of all words with the given aspect term as their head\n",
    "def find_aspect_tails(sent, aspect_id):\n",
    "    aspect_tails = []\n",
    "    for sentence in sent.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.head == aspect_id:\n",
    "                aspect_tails.append(word.id)\n",
    "    return aspect_tails\n",
    "\n",
    "#a function that takes a selection of word ids in a sentence, gets their lemma form and upos tag.\n",
    "#also converts their upos tag into a POS tag that SWN can utilise.\n",
    "#returns a list of triples, one for each word id originally provided. Each triple lists the lemma, pos-tag and id for that word.\n",
    "def get_lemmas_and_types(sent, ids):\n",
    "    lemmas = []\n",
    "    for sentence in sent.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.id in ids:\n",
    "                lemmas.append([word.lemma, word.upos, word.id])\n",
    "    #convert the lemmas into a form SWN can recognise\n",
    "    for lemma in lemmas:\n",
    "        if lemma[1] == 'ADJ':\n",
    "            lemma[1] = 's'\n",
    "        elif lemma[1] == 'ADV':\n",
    "            lemma[1] = 'r'\n",
    "        elif lemma[1] == 'VERB':\n",
    "            lemma[1] = 'v'\n",
    "        elif lemma[1] == 'NOUN':\n",
    "            lemma[1] = 'n'\n",
    "        else:\n",
    "            lemma[1] = 'n'\n",
    "    return lemmas\n",
    "\n",
    "#a function for scaling sentiment scores, putting them onto a -1.0 to +1.0 scale.\n",
    "def scale_score(min_score, max_score, score):\n",
    "    scaled_score = 2 * ((score - min_score) / (max_score - min_score)) - 1\n",
    "    return scaled_score\n",
    "\n",
    "#a function that takes the lemma and type of a word in a sentence, and checks its SWN score.\n",
    "#returns the positive and negative scores of that word.\n",
    "def get_swn_scores(sent, lemma, word_type):\n",
    "    tokenized_sent = []\n",
    "    for i, sentence in enumerate(sent.sentences):\n",
    "        for token in sentence.tokens:\n",
    "            tokenized_sent.append(token.text)\n",
    "    disambig = str(nltk.wsd.lesk(tokenized_sent, lemma, word_type))\n",
    "    #if the disambiguation fails, it is likely that an 's' type is actually an 'a' type in SWN, so try that:\n",
    "    if disambig == 'None':\n",
    "        disambig = str(nltk.wsd.lesk(tokenized_sent, lemma, 'a'))\n",
    "    #if still nothing, then just set it to 0.0\n",
    "    if disambig == 'None':\n",
    "        pos = 0.0\n",
    "        neg = 0.0\n",
    "    #if the disambiguation worked, we take the relevant part of the result and get its positive and negative scores from SWN\n",
    "    else:\n",
    "        disambig_slice = disambig[8:-2]\n",
    "        senti_details = swn.senti_synset(disambig_slice)\n",
    "        pos = senti_details.pos_score()\n",
    "        neg = senti_details.neg_score()\n",
    "    return pos, neg\n",
    "\n",
    "#a function to check for all words linked by the parser to a sentiment term, to check for modifiers\n",
    "def get_sentiment_links(sent, sentiment_id):\n",
    "    sentiment_links = []\n",
    "    sent_head = None\n",
    "    for sentence in sent.sentences:\n",
    "        for word in sentence.words:\n",
    "            if word.id == sentiment_id:\n",
    "                sent_head = word.head\n",
    "        for word in sentence.words:\n",
    "            if word.head == sentiment_id or word.id == sent_head:\n",
    "                sentiment_links.append(word.text)\n",
    "    return sentiment_links\n",
    "\n",
    "#a function to replace a term within a sentence, used to handle multi-word/hyphenated modifiers and new sentiment terms\n",
    "def multi_word_slice(sentence, term, new_term):\n",
    "    first_half = sentence[:sentence.index(term)]\n",
    "    second_half = sentence[sentence.index(term)+len(term):]\n",
    "    sliced_sent = first_half + new_term + second_half\n",
    "    return sliced_sent\n",
    "\n",
    "#the main function that scores all the aspects within a text\n",
    "#takes the full review text as input, along with three parameters:\n",
    "#1.) intro_to_ignore defines what percentage (always rounding down) of the sentences at the start to consider the 'introduction', and to consequently ignore\n",
    "#2.) iem_weights, which defines what percentage weight to assign to the implicit aspect, explicit aspects (collectively) and unconnected sentiment, respectively. Must sum to 100.\n",
    "#3.) the mode, which defines the mode to run the function in and affects its outputs. Can be 1, 2, 3, 4 or 5.\n",
    "#mode 1 = verbose, printing a lot of information as it runs, and at the end\n",
    "#mode 2 = default, makes and returns a rating prediction\n",
    "#mode 3 = detailed aspect polarity outputs, returns a dataframe detailing all the aspect scores across each sentence\n",
    "#mode 4 = emphasis on the weightiest sentences, returns the indices of the most important sentences as this system sees it\n",
    "#mode 5 = emphasis on rationales, returns the weightiest sentences\n",
    "def score_aspects(full_review, intro_to_ignore=8, iem_weights=[40, 40, 20], mode=2):\n",
    "    #check to ensure the weights sum to 100\n",
    "    if iem_weights[0] + iem_weights[1] + iem_weights[2] != 100:\n",
    "        print(\"Error: weights must sum to 100.\")\n",
    "        return\n",
    "    #tokenize the review into sentences so they can be worked through one by one\n",
    "    review = nltk.sent_tokenize(full_review)\n",
    "    #count the number of sentences\n",
    "    num_sents = len(review)\n",
    "    #remove the intro if specified\n",
    "    if intro_to_ignore > 0:\n",
    "        intro_sents = math.floor(num_sents/(100/intro_to_ignore))\n",
    "        review = review[intro_sents:]\n",
    "    #create a dictionary for recording the sentiment score of each aspect within each sentence, if needed for mode 3\n",
    "    if mode == 3:\n",
    "        sentence_details_lists = [ ([0] * 20) for x in range(len(review)) ]\n",
    "        aspect_column_refs = {'guitars': 0, 'vocals': 1, 'drums': 2, 'production': 3, 'breakdowns': 4, 'lyrics': 5, 'ambience': 6, 'writing': 7, 'bass': 8, 'melodies': 9, 'technicality': 10, 'keyboards': 11, 'symphonics': 12, 'exotic instruments': 13, 'creativity': 14, 'comedy': 15, 'catchiness': 16, 'implicit': 17, 'unattached': 18}\n",
    "    #dictionary for recording sentiment scores across review\n",
    "    aspect_scores = {'implicit': 0, 'guitars': 0, 'vocals': 0, 'drums': 0, 'production': 0, 'breakdowns': 0, 'lyrics': 0, 'ambience': 0, 'writing': 0, 'bass': 0, 'melodies': 0, 'technicality': 0, 'keyboards': 0, 'symphonics': 0, 'exotic instruments': 0, 'creativity': 0, 'comedy': 0, 'catchiness': 0}\n",
    "    total_unconnected_sentiment = 0\n",
    "    #start recording sentence number, beginning at 0\n",
    "    current_sent = 0\n",
    "    #create a list to store all the sentence scores, to later pull up the weightiest sentences\n",
    "    all_sents = []\n",
    "    for sentence in review:\n",
    "        if mode == 1:\n",
    "            print(\"Sentence being evaluated:\\n\",sentence,\"\\n\")\n",
    "        #modify terms within each sentence if needed\n",
    "        for term in words_to_modify.keys():\n",
    "            if term in sentence:\n",
    "                new_term = words_to_modify[term]\n",
    "                sentence = multi_word_slice(sentence, term, new_term)\n",
    "        #increment sentence index number\n",
    "        current_sent += 1\n",
    "        sentence_score = [current_sent, 0]\n",
    "        #count the number of aspect terms within the sentence\n",
    "        t_sentence = nlp(sentence)\n",
    "        ids_range = range((t_sentence.num_tokens)+1)\n",
    "        #we also add all word ids to not_checked, which will be updated as we go, so that we can use to check for unconnected sentiment terms after aspects are all checked\n",
    "        not_checked = []\n",
    "        for i in ids_range[1:]:\n",
    "            not_checked.append(i)\n",
    "        aspect_ids = aspect_detection(t_sentence)\n",
    "        #if there aren't any, move on; otherwise:\n",
    "        if len(aspect_ids) > 0:\n",
    "            if mode == 1:\n",
    "                print(\"Aspects detected.\")\n",
    "            #work through each set of aspect details found (as there may be multiple aspects in a single sentence)\n",
    "            for aspect_details in aspect_ids:\n",
    "                #see which aspect category this word belongs to\n",
    "                aspect_name = aspect_category(aspect_details[1])\n",
    "                #note its head and dependent terms (tails)\n",
    "                aspect_head = [aspect_details[2]]\n",
    "                aspect_tails = find_aspect_tails(t_sentence, aspect_details[0])\n",
    "                #this gives us the ids of all connected words to this aspect term:\n",
    "                all_linked = aspect_head + aspect_tails\n",
    "                #if the aspect_ids list is length 4, then it found a compound, and the last value is the head of the head\n",
    "                if len(aspect_details) == 4:\n",
    "                    #so we add this, and then its tails, to the full list of potentially relevant indexes\n",
    "                    all_linked.append(aspect_details[3])\n",
    "                    compound_tails = find_aspect_tails(t_sentence, aspect_details[3])\n",
    "                    for i in compound_tails:\n",
    "                        all_linked.append(i)\n",
    "                #we also check if the aspect term itself is within the \"linked\" list, and remove it if so\n",
    "                if aspect_details[0] in all_linked:\n",
    "                    all_linked.remove(aspect_details[0])\n",
    "                #not_checked is updated to exclude the ids of words that are linked to sentiment terms\n",
    "                not_checked = list(set(not_checked)^set(all_linked))\n",
    "                #we then get the lemmas of all linked words, to check for their SWN scores\n",
    "                lemmas = get_lemmas_and_types(t_sentence, all_linked)\n",
    "                #the score for this aspect in this sentence starts at 0\n",
    "                sentence_aspect_score = 0\n",
    "                #we can then work through the details of all connected words\n",
    "                sentiment_terms = []\n",
    "                for lemma in lemmas:\n",
    "                    lemma_score = 0\n",
    "                    #if the word exists in the intercept layer, then use the sentiment score there\n",
    "                    if lemma[0] in new_sentiment_terms.keys():\n",
    "                        sentiment_terms.append(lemma[0])\n",
    "                        lemma_score = new_sentiment_terms[lemma[0]]\n",
    "                        #also check for linked modifiers\n",
    "                        sentiment_links = get_sentiment_links(t_sentence, lemma[2])\n",
    "                        modifier = 0.0\n",
    "                        for i in sentiment_links:\n",
    "                            if i in modifiers.keys():\n",
    "                                modifier += modifiers[i]\n",
    "                        lemma_score = lemma_score + (lemma_score * modifier)\n",
    "                    #if the word is not in the intercept layer, then check for it in SWN\n",
    "                    else:\n",
    "                        synset_check = list(swn.senti_synsets(lemma[0]))\n",
    "                        if len(synset_check) > 0:\n",
    "                            sentiment_terms.append(lemma[0])\n",
    "                            pos_score, neg_score = get_swn_scores(t_sentence, lemma[0], lemma[1])\n",
    "                            lemma_score = (pos_score - neg_score)\n",
    "                            if mode == 1:\n",
    "                                print(\"The term '\",lemma[0],\"' yields a score difference of\",lemma_score)\n",
    "                            #also check for linked modifiers\n",
    "                            sentiment_links = get_sentiment_links(t_sentence, lemma[2])\n",
    "                            modifier = 0.0\n",
    "                            for i in sentiment_links:\n",
    "                                if i in modifiers.keys():\n",
    "                                    modifier += modifiers[i]\n",
    "                            lemma_score = lemma_score + (lemma_score * modifier)\n",
    "                    #either way, we end up with a score for this particular word, which we will use to update the aspect score and the overall sentence score\n",
    "                    sentence_aspect_score += lemma_score\n",
    "                    sentence_score[1] += lemma_score\n",
    "                #we now have a total score for this aspect instance in this sentence\n",
    "                #if we're in mode 3, we also need to update the dataframe recording these details\n",
    "                if mode == 3:\n",
    "                    column = aspect_column_refs[aspect_name]\n",
    "                    sentence_details_lists[(current_sent-1)][column] += sentence_aspect_score\n",
    "                #modify the appropriate part of the aspect_scores dictionary with the score\n",
    "                aspect_scores[aspect_name] += sentence_aspect_score\n",
    "                if mode == 1:\n",
    "                    print(\"CATEGORY TYPE:\\t\\t\",aspect_name,\"\\nASPECT KEYWORD:\\t\\t'\",aspect_details[1],\"'\\nSCORE ASSIGNED:\\t\\t\",sentence_aspect_score,\"\\nLINKED SENTIMENT TERMS:\\n\",sentiment_terms,\"\\n\")\n",
    "        else:\n",
    "            if mode == 1:\n",
    "                print(\"No aspects detected in the sentence.\")\n",
    "        #once we have checked for all aspects (implicit and explicit), we pass through for any remaining unconnected sentiment in the sentence\n",
    "        additional_lemmas = get_lemmas_and_types(t_sentence, not_checked)\n",
    "        unconnected_sentence_score = 0\n",
    "        #we perform the same basic process as for the aspects, just now for unconnected sentiment\n",
    "        for adlem in additional_lemmas:\n",
    "            lemma_score = 0\n",
    "            if adlem[0] in new_sentiment_terms.keys():\n",
    "                lemma_score = new_sentiment_terms[adlem[0]]\n",
    "                if mode == 1:\n",
    "                        print(\"The UNCONNECTED term '\",adlem[0],\"' yields a score difference of\",lemma_score)\n",
    "                sentiment_links = get_sentiment_links(t_sentence, adlem[2])\n",
    "                modifier = 0.0\n",
    "                for i in sentiment_links:\n",
    "                    if i in modifiers.keys():\n",
    "                        modifier += modifiers[i]\n",
    "                lemma_score = lemma_score + (lemma_score * modifier)\n",
    "                if mode == 1:\n",
    "                        print(\"\\tWith modification, this becomes:\",lemma_score)\n",
    "            else:\n",
    "                synset_check = list(swn.senti_synsets(adlem[0]))\n",
    "                if len(synset_check) > 0:\n",
    "                    pos_score, neg_score = get_swn_scores(t_sentence, adlem[0], adlem[1])\n",
    "                    lemma_score = (pos_score - neg_score)\n",
    "                    if mode == 1:\n",
    "                        print(\"The UNCONNECTED term '\",adlem[0],\"' yields a score difference of\",lemma_score)\n",
    "                    sentiment_links = get_sentiment_links(t_sentence, adlem[2])\n",
    "                    modifier = 0.0\n",
    "                    for i in sentiment_links:\n",
    "                        if i in modifiers.keys():\n",
    "                            modifier += modifiers[i]\n",
    "                    lemma_score = lemma_score + (lemma_score * modifier)\n",
    "                    if mode == 1:\n",
    "                        print(\"With modification, this becomes:\",lemma_score)\n",
    "            unconnected_sentence_score += lemma_score\n",
    "        sentence_score[1] += unconnected_sentence_score\n",
    "        all_sents.append(sentence_score)\n",
    "        total_unconnected_sentiment += unconnected_sentence_score\n",
    "        if mode == 3:\n",
    "            sentence_details_lists[(current_sent-1)][18] += unconnected_sentence_score\n",
    "            sentence_details_lists[(current_sent-1)][19] = sum(sentence_details_lists[(current_sent-1)][0:19])\n",
    "        if mode == 1:\n",
    "            print(\"\\nThe value of the unconnected sentiment in this sentence is:\",unconnected_sentence_score,\"\\n\")\n",
    "            print(\"\\nThe total sentiment value of this sentence is:\",sentence_score[1])\n",
    "            print(\"==============================================================\")\n",
    "    #having gone through every sentence, we now need to see which had the greatest weight\n",
    "    all_weights = []\n",
    "    for sent in all_sents:\n",
    "        weight = abs(0.0 - sent[1])\n",
    "        all_weights.append(weight)\n",
    "    #find the index values of the three biggest weights\n",
    "    biggest = sorted(range(len(all_weights)), key = lambda sub: all_weights[sub])[-3:]\n",
    "    if mode == 1:\n",
    "        if len(biggest) == 0:\n",
    "            print(\"No aspect sentiment found!\")\n",
    "        else:\n",
    "            print(\"\\nThe sentences with the strongest impact on sentiment were:\")\n",
    "            for i in biggest:\n",
    "                print(\"Sentence:\",review[(all_sents[i][0])-1],\"\\nScore:\",all_sents[i][1],\"\\n\")\n",
    "    #remove implicit from the aspects list, as it will now need to be handled differently\n",
    "    imp_score = aspect_scores['implicit']\n",
    "    del aspect_scores['implicit']\n",
    "    #go through the implicit, explicit and miscellaneous aspects, limiting them to their max/min\n",
    "    for key, value in aspect_scores.items():\n",
    "        if value > 2:\n",
    "            aspect_scores[key] = 2\n",
    "        elif value < -2:\n",
    "            aspect_scores[key] = -2\n",
    "    if imp_score > 4:\n",
    "        imp_score = 4\n",
    "    elif imp_score < -4:\n",
    "        imp_score = -4\n",
    "    if total_unconnected_sentiment > 4:\n",
    "        total_unconnected_sentiment = 4\n",
    "    elif total_unconnected_sentiment < -4:\n",
    "        total_unconnected_sentiment = -4\n",
    "    #scale all scores, putting them all on a -1 to +1 scale\n",
    "    scaled_imp = scale_score(-4, 4, imp_score)\n",
    "    scaled_unconnected = scale_score(-4, 4, total_unconnected_sentiment)\n",
    "    num_scored_aspects = 0\n",
    "    scores_range = []\n",
    "    scaled_scores = []\n",
    "    for score in aspect_scores.values():\n",
    "        scores_range.append(score)\n",
    "        if score != 0:\n",
    "            num_scored_aspects += 1\n",
    "    for score in scores_range:\n",
    "        scaled = scale_score(-2, 2, score)\n",
    "        if num_scored_aspects > 0:\n",
    "            scaled_scores.append(scaled/num_scored_aspects)\n",
    "        else:\n",
    "            scaled_scores.append(scaled/1)\n",
    "    #convert each scaled value into a weight for the overall rating prediction\n",
    "    scaled_imp = (scaled_imp/100)*iem_weights[0]\n",
    "    scaled_exp = ((sum(scaled_scores))/100)*iem_weights[1]\n",
    "    scaled_misc = (scaled_unconnected/100)*iem_weights[2]\n",
    "    #sum up the weights to get a full score, on the -1 to +1 scale\n",
    "    final_score = scaled_imp + scaled_exp + scaled_misc\n",
    "    #convert into /10 rating\n",
    "    predicted_rating = (final_score + 1) * 5\n",
    "    #print and return results, depending on mode\n",
    "    if mode == 1:\n",
    "        print(\"The predicted score for this review is\",final_score,\", which translates to a review rating of\",round(predicted_rating, 1))\n",
    "        print(\"\\nAll scores were as follows:\",aspect_scores)\n",
    "        print(\"The number of scored explicit aspects was:\",num_scored_aspects)\n",
    "        print(\"\\nThe implicit aspect rating was\",imp_score)\n",
    "        print(\"The unconnected sentiment was worth\",total_unconnected_sentiment)\n",
    "    if mode == 2:\n",
    "        return round(predicted_rating, 1)\n",
    "    if mode == 3:\n",
    "        #NOTE: All values shown in this table are before limits, scaling and weighting\n",
    "        df = pd.DataFrame.from_records(sentence_details_lists)\n",
    "        columns_list = ['guitars', 'vocals', 'drums', 'production', 'breakdowns', 'lyrics', 'ambience', 'writing', 'bass', 'melodies', 'technicality', 'keyboards', 'symphonics', 'exotic instruments', 'creativity', 'comedy', 'catchiness', 'implicit', 'unattached', 'SENTENCE TOTAL']\n",
    "        df.columns = columns_list\n",
    "        totals = {'guitars': 0, 'vocals': 0, 'drums': 0, 'production': 0, 'breakdowns': 0, 'lyrics': 0, 'ambience': 0, 'writing': 0, 'bass': 0, 'melodies': 0, 'technicality': 0, 'keyboards': 0, 'symphonics': 0, 'exotic instruments': 0, 'creativity': 0, 'comedy': 0, 'catchiness': 0, 'implicit': 0, 'unattached': 0, 'SENTENCE TOTAL': 0}\n",
    "        for i in columns_list:\n",
    "            totals[i] = df[i].sum()\n",
    "        df = df.append(totals, ignore_index=True)\n",
    "        print(df)\n",
    "        return df\n",
    "    if mode == 4:\n",
    "        print(\"The indices of the sentences with the biggest impact were:\",biggest,\"\\nNote: remember that the first sentence is index 0.\")\n",
    "        return biggest\n",
    "    if mode == 5:\n",
    "        rationales = []\n",
    "        for i in biggest:\n",
    "            rationales.append(review[(all_sents[i][0])-1])\n",
    "        return rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to conduct the score_aspects function across a batch of reviews.\n",
    "#this takes the same inputs as score_aspects, except that it takes a dataframe including a 'Review' column containing the review texts (and may need other columns depending on the mode it's run in), and a filename to give to an output file\n",
    "#the modes generally match to the modes of score_aspects, collecting the outputs of score_aspects (in that mode) into an output for this function\n",
    "#if run in mode 2 (the default), it will attempt to check the result metrics with the check_metrics function\n",
    "\n",
    "def review_batch_aspects(all_reviews, mode=2, intro_to_ignore=8, iem_weights=[40, 40, 20], filename='aspectresults.csv'):\n",
    "    review_num = 0\n",
    "    all_predictions = []\n",
    "    all_actual_ratings = []\n",
    "    all_ids = []\n",
    "    all_titles = []\n",
    "    all_sent_weights = []\n",
    "    all_rationales = []\n",
    "    all_review_texts = []\n",
    "    for index, row in all_reviews.iterrows():\n",
    "        review_text = row['Review']\n",
    "        review_ID = row['ID']\n",
    "        if mode == 2:\n",
    "            print(\"Processing review\",index)\n",
    "            pred = score_aspects(full_review=review_text, intro_to_ignore=intro_to_ignore, iem_weights=iem_weights, mode=2)\n",
    "            all_predictions.append(pred)\n",
    "            actual_rating = row['Rating']\n",
    "            all_actual_ratings.append(actual_rating)\n",
    "            review_id = row['ID']\n",
    "            all_ids.append(review_id)\n",
    "            title = row['Title']\n",
    "            all_titles.append(title)\n",
    "        elif mode == 3:\n",
    "            review_num += 1\n",
    "            all_details = score_aspects(full_review=review_text, intro_to_ignore=intro_to_ignore, iem_weights=iem_weights, mode=3)\n",
    "            title = 'review' + str(review_ID) + '.csv'\n",
    "            all_details.to_csv(title, index=True, header=True)\n",
    "        elif mode == 4:\n",
    "            print(\"Processing review\",index)\n",
    "            weightiest_sents = score_aspects(full_review=review_text, intro_to_ignore=intro_to_ignore, iem_weights=iem_weights, mode=4)\n",
    "            all_sent_weights.append(weightiest_sents)\n",
    "            all_ids.append(review_ID)\n",
    "            title = row['Title']\n",
    "            all_titles.append(title)\n",
    "        elif mode == 5:\n",
    "            print(\"Processing review\",index)\n",
    "            rationales = score_aspects(full_review=review_text, intro_to_ignore=intro_to_ignore, iem_weights=iem_weights, mode=5)\n",
    "            all_rationales.append(rationales)\n",
    "            all_ids.append(review_ID)\n",
    "            all_review_texts.append(review_text)\n",
    "    if mode == 2:\n",
    "        results = pd.DataFrame(list(zip(all_ids, all_titles, all_actual_ratings, all_predictions)), columns =['ID', 'Title', 'Actual rating', 'Prediction'])\n",
    "        results.to_csv(filename)\n",
    "        mape, rmse, rho, pval = check_metrics(all_actual_ratings, all_predictions, mode='return')\n",
    "        print(\"The following results were achieved:\\n\\tMAPE:\",mape,\"\\n\\tRMSE:\",rmse,\"\\n\\tRHO:\",rho,\"\\n\\tPVAL:\",pval)\n",
    "        return all_actual_ratings, all_predictions\n",
    "    elif mode == 4:\n",
    "        results = pd.DataFrame(list(zip(all_ids, all_titles, all_sent_weights)), columns =['ID', 'Title', 'Weightiest sentences'])\n",
    "        results.to_csv(filename)\n",
    "        return all_sent_weights\n",
    "    elif mode == 5:\n",
    "        results = pd.DataFrame(list(zip(all_ids, all_review_texts, all_rationales)), columns = ['ID', 'Review', 'Rationales'])\n",
    "        results.to_csv(filename)\n",
    "        return all_rationales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57367e1c",
   "metadata": {},
   "source": [
    "## Metrics and hyperparameter tuning\n",
    "This section contains functions for checking the metrics of the systems, as well as tuning the hyperparameters of the rule-based system (the simplistic system requires no tuning and BERT has its own methods for that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17960f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for calculating MAPE, RMSE, Spearman's rho correlation coefficient and p-value\n",
    "#note that MAPE cannot handle actual values of 0.0, so if any such entries exist, they should be removed first\n",
    "\n",
    "def calculate_rmse(errors):\n",
    "    sum_squared_errors = 0\n",
    "    for error in errors:\n",
    "        squared_error = error*error\n",
    "        sum_squared_errors += squared_error\n",
    "    mean_squared_errors = sum_squared_errors/len(errors)\n",
    "    rmse = math.sqrt(mean_squared_errors)\n",
    "    return rmse\n",
    "\n",
    "def calculate_mape(errors, actual_values):\n",
    "    ape = []\n",
    "    for i, error in enumerate(errors):\n",
    "        flipped = error*(-1)\n",
    "        ape.append(abs(flipped/actual_values[i]))\n",
    "    mape = sum(ape)/len(ape)\n",
    "    return mape\n",
    "\n",
    "def check_metrics(actual_values, predictions, mode='verbose'):\n",
    "    errors = []\n",
    "    for i, x in enumerate(predictions):\n",
    "        errors.append(x - actual_values[i])\n",
    "    mape = calculate_mape(errors, actual_values)\n",
    "    rmse = calculate_rmse(errors)\n",
    "    rho, pval = stats.spearmanr(actual_values, predictions)\n",
    "    if mode == 'verbose':\n",
    "        print(\"The mean absolute percentage error value is:\",mape)\n",
    "        print(\"The root mean squared error value is:\",rmse)\n",
    "        print(\"The Spearman's rho correlation coefficient value is:\",rho)\n",
    "        print(\"The p-value is:\",pval)\n",
    "    elif mode == 'return':\n",
    "        return mape, rmse, rho, pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a grid search function that checks different possible intro_to_ignore values\n",
    "\n",
    "def hypersearch_intro(validation_set, param_range):\n",
    "    all_results = []\n",
    "    for param_value in param_range:\n",
    "        print(\"Now checking this value:\",param_value)\n",
    "        all_preds = []\n",
    "        all_actual_ratings = []\n",
    "        for index, row in validation_set.iterrows():\n",
    "            print(\"Processing review\",index)\n",
    "            review_text = row['Review']\n",
    "            actual_rating = row['Rating']\n",
    "            prediction = score_aspects(full_review=review_text, intro_to_ignore=param_value, mode=2)\n",
    "            all_preds.append(prediction)\n",
    "            all_actual_ratings.append(actual_rating)\n",
    "        mape, rmse, rho, pval = check_metrics(all_actual_ratings, all_preds, mode='return')\n",
    "        all_results.append([param_value, [mape, rmse, rho, pval]])\n",
    "        print(\"\\nFor this intro-to-ignore value:\",param_value,\", the following results were achieved:\\n\\tMAPE:\",mape,\"\\n\\tRMSE:\",rmse,\"\\n\\tRHO:\",rho,\"\\n\\tPVAL:\",pval)\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3320555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a grid search function that checks different implicit/explicit/miscellaneous weights values\n",
    "\n",
    "def hypersearch_weights(validation_set, param_range):\n",
    "    all_results = []\n",
    "    for param_value in param_range:\n",
    "        print(\"Now checking this value:\",param_value)\n",
    "        all_preds = []\n",
    "        all_actual_ratings = []\n",
    "        mape = None\n",
    "        rmse = None\n",
    "        rho = None\n",
    "        pval = None\n",
    "        for index, row in validation_set.iterrows():\n",
    "            print(\"Processing review\",index)\n",
    "            review_text = row['Review']\n",
    "            actual_rating = row['Rating']\n",
    "            prediction = score_aspects(full_review=review_text, intro_to_ignore=8, iem_weights=param_value, mode=2)\n",
    "            all_preds.append(prediction)\n",
    "            all_actual_ratings.append(actual_rating)\n",
    "            #print(\"Sanity check - prediction:\",prediction,\"and actual score:\",actual_rating)\n",
    "        print(\"Sanity check - The predictions being passed into check_metrics are:\",all_preds)\n",
    "        mape, rmse, rho, pval = check_metrics(all_actual_ratings, all_preds, mode='return')\n",
    "        all_results.append([param_value, [mape, rmse, rho, pval]])\n",
    "        print(\"\\nFor this weights distribution:\",param_value,\", the following results were achieved:\\n\\tMAPE:\",mape,\"\\n\\tRMSE:\",rmse,\"\\n\\tRHO:\",rho,\"\\n\\tPVAL:\",pval)\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026c956",
   "metadata": {},
   "source": [
    "## Simplistic system\n",
    "This section contains the simplistic sentiment counting system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff0cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function that takes a simpler approach\n",
    "#this function just checks every word, disambiguates it and checks it against the intercept layer and SWN, and counts up all the sentiment scores in this fashion\n",
    "\n",
    "def brute_force_count(full_review):\n",
    "    review = nltk.sent_tokenize(full_review)\n",
    "    total_sentiment = 0\n",
    "    for sentence in review:\n",
    "        for term in words_to_modify.keys():\n",
    "            if term in sentence:\n",
    "                new_term = words_to_modify[term]\n",
    "                sentence = multi_word_slice(sentence, term, new_term)\n",
    "        t_sentence = nlp(sentence)\n",
    "        for sent in t_sentence.sentences:\n",
    "            for word in sent.words:\n",
    "                score = 0\n",
    "                if word.lemma in new_sentiment_terms.keys():\n",
    "                    score = new_sentiment_terms[word.lemma]\n",
    "                elif len(list(swn.senti_synsets(word.lemma))) > 0:\n",
    "                    if word.upos == 'ADJ':\n",
    "                        word_type = 's'\n",
    "                    elif word.upos == 'ADV':\n",
    "                        word_type = 'r'\n",
    "                    elif word.upos == 'VERB':\n",
    "                        word_type = 'v'\n",
    "                    elif word.upos == 'NOUN':\n",
    "                        word_type = 'n'\n",
    "                    else:\n",
    "                        word_type = 'n'\n",
    "                    pos_score, neg_score = get_swn_scores(t_sentence, word.lemma, word_type)\n",
    "                    score = (pos_score - neg_score)\n",
    "                total_sentiment += score\n",
    "    if total_sentiment > 10:\n",
    "        total_sentiment = 10\n",
    "    elif total_sentiment < -10:\n",
    "        total_sentiment = -10\n",
    "    scaled_score = scale_score(-10, 10, total_sentiment)\n",
    "    predicted_rating = (scaled_score + 1) * 5\n",
    "    return round(predicted_rating, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7267e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function for running the simplistic count across a batch of reviews, and outputting the results as a csv file\n",
    "#it will attempt to check the result metrics with the check_metrics function\n",
    "\n",
    "def review_batch_brute(all_reviews, filename='bruteforceresults.csv'):\n",
    "    review_num = 0\n",
    "    all_predictions = []\n",
    "    all_actual_ratings = []\n",
    "    all_ids = []\n",
    "    all_titles = []\n",
    "    for index, row in all_reviews.iterrows():\n",
    "        print(\"Processing review\",index)\n",
    "        review_text = row['Review']\n",
    "        pred = brute_force_count(review_text)\n",
    "        all_predictions.append(pred)\n",
    "        actual_rating = row['Rating']\n",
    "        all_actual_ratings.append(actual_rating)\n",
    "        review_id = row['ID']\n",
    "        all_ids.append(review_id)\n",
    "        title = row['Title']\n",
    "        all_titles.append(title)\n",
    "    results = pd.DataFrame(list(zip(all_ids, all_titles, all_actual_ratings, all_predictions)), columns =['ID', 'Title', 'Actual rating', 'Prediction'])\n",
    "    results.to_csv(filename)\n",
    "    mape, rmse, rho, pval = check_metrics(all_actual_ratings, all_predictions, mode='return')\n",
    "    print(\"The following results were achieved:\\n\\tMAPE:\",mape,\"\\n\\tRMSE:\",rmse,\"\\n\\tRHO:\",rho,\"\\n\\tPVAL:\",pval)\n",
    "    return all_actual_ratings, all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b78876",
   "metadata": {},
   "source": [
    "## Preparing files for BERT script\n",
    "This section contains code for preparing files for the BERT script. Note that the methods needed may vary significantly depending on exactly how you implement BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f81618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT may require some additional characters to be changed in order to function properly\n",
    "#in this case, these characters were simply removed for the sake of time\n",
    "#but be careful not to perform this on the original dataset, as this will lose information unnecessarily\n",
    "#instead, perform this on copies specifically set aside for BERT\n",
    "\n",
    "fixdict2 = {\n",
    "    '\\n': ' ',\n",
    "    '\\r': ' ',\n",
    "    '\\t': ' ',\n",
    "    'И': '',\n",
    "    'я': '',\n",
    "    'м': '',\n",
    "    'н': '',\n",
    "    'Л': '',\n",
    "    'г': '',\n",
    "    'и': '',\n",
    "    '噤': '',\n",
    "    '夢': '',\n",
    "    'γ': '',\n",
    "    '島': '',\n",
    "    '嶼': '',\n",
    "    '神': '',\n",
    "    '話': '',\n",
    "    '暮': '',\n",
    "    '山': '',\n",
    "    '船': '',\n",
    "    '影': '',\n",
    "    '孤': '',\n",
    "    '燈': '',\n",
    "    '微': '',\n",
    "    '雁': '',\n",
    "    '紛': '',\n",
    "    '飛': '',\n",
    "    '灰': '',\n",
    "    '月': '',\n",
    "    '漸': '',\n",
    "    '明': '',\n",
    "    '念': '',\n",
    "    '伊': '',\n",
    "    '人': '',\n",
    "    '現': '',\n",
    "    '象': '',\n",
    "    '鷹': '',\n",
    "    '行': '',\n",
    "    '創': '',\n",
    "    '世': '',\n",
    "    '洪': '',\n",
    "    '水': '',\n",
    "    '巨': '',\n",
    "    '饒': '',\n",
    "    '亞': '',\n",
    "    '哲': '',\n",
    "    '鬼': '',\n",
    "    'ḥ': '',\n",
    "    '𒀭': '',\n",
    "    'ṭ': '',\n",
    "    '〇': '',\n",
    "    'ẁ': '',\n",
    "    '∞': '',\n",
    "    'वा': '',\n",
    "    'घ': '',\n",
    "    'न': '',\n",
    "    'ख': '',\n",
    "    'χ': '',\n",
    "    'ι': '',\n",
    "    'π': '',\n",
    "    'λ': '',\n",
    "    'η': '',\n",
    "    'σ': '',\n",
    "    'Ř': '',\n",
    "    '椅': '',\n",
    "    '子': '',\n",
    "    'ב': '',\n",
    "    'Л': '',\n",
    "    'г': '',\n",
    "    'н': '',\n",
    "    'д': '',\n",
    "    'Д': '',\n",
    "    '집': '',\n",
    "    'τ': '',\n",
    "    'ε': '',\n",
    "    'ρ': '',\n",
    "    'φ': '',\n",
    "    'ω': '',\n",
    "    'ς': '',\n",
    "    'ツ': '',\n",
    "    '颠': '',\n",
    "    '覆': '',\n",
    "    '✓': '',\n",
    "    \"•\": '',\n",
    "    \"Γ\": '',\n",
    "    \"Φ\": '',\n",
    "    \"Ξ\": '',\n",
    "    '': '',\n",
    "    'є': '',\n",
    "    '間': '',\n",
    "    '无': '',\n",
    "    '名': '',\n",
    "    'Δ': '',\n",
    "    'α': '',\n",
    "    'μ': '',\n",
    "    'υ': '',\n",
    "    'ǫ': '',\n",
    "    'φ': '',\n",
    "    'ş': '',\n",
    "    'к': '',\n",
    "    'ю': '',\n",
    "    'ч': '',\n",
    "    '期': '',\n",
    "    '会': '',\n",
    "    'τ': '',\n",
    "    'ρ': '',\n",
    "    'ω': '',\n",
    "    'α': '',\n",
    "    'π': '',\n",
    "    '∞': '',\n",
    "    '╪': '',\n",
    "    'ʊ': '',\n",
    "    'Λ': '',\n",
    "    'ή': '',\n",
    "    'η': '',\n",
    "    'Θ': '',\n",
    "    'κ': '',\n",
    "    'β': '',\n",
    "    'Δ': '',\n",
    "    'δ': '',\n",
    "    'ά': '',\n",
    "    'ἅ': '',\n",
    "    'ἄ': '',\n",
    "    'ὠ': '',\n",
    "    'λ': '',\n",
    "    'ι': '',\n",
    "    'Б': '',\n",
    "    'Ω': '',\n",
    "    'т': '',\n",
    "    'ł': '',\n",
    "    'Ш': '',\n",
    "    'й': '',\n",
    "    'Ι': '',\n",
    "    'σ': '',\n",
    "    'ό': '',\n",
    "    'θ': '',\n",
    "    'ε': '',\n",
    "    'ς': '',\n",
    "    'э': '',\n",
    "    'п': '',\n",
    "    'б': '',\n",
    "    'л': '',\n",
    "    'д': '',\n",
    "    'З': '',\n",
    "    'ы': '',\n",
    "    'П': '',\n",
    "    'щ': '',\n",
    "    'ш': '',\n",
    "    'Г': '',\n",
    "    'ж': '',\n",
    "    'Ч': '',\n",
    "    'ъ': '',\n",
    "    'ξ': '',\n",
    "    'ζ': '',\n",
    "    'ɔ': '',\n",
    "    'Ş': '',\n",
    "    'ύ': '',\n",
    "    'Σ': '',\n",
    "    'Π': '',\n",
    "    'Ή': '',\n",
    "    'ώ': '',\n",
    "    'έ': '',\n",
    "    'μ': '',\n",
    "    'χ': '',\n",
    "    'Ș': '',\n",
    "    'Ж': '',\n",
    "    'ц': '',\n",
    "    'Я': '',\n",
    "    'ű': '',\n",
    "    'Ά': '',\n",
    "    'ψ': '',\n",
    "    'Ł': '',\n",
    "    'ф': '',\n",
    "    'ᗅ': '',\n",
    "    'ᗺ': '',\n",
    "    'ᗷ': '',\n",
    "    'Ţ': '',\n",
    "    '♪': '',\n",
    "    '♫': '',\n",
    "    '☑': '',\n",
    "    'Ц': '',\n",
    "    'з': '',\n",
    "    'Х': '',\n",
    "    'в': '',\n",
    "    'ь': '',\n",
    "    'œ': '',\n",
    "    'Þ': '',\n",
    "    'к': '',\n",
    "    'Ό': '',\n",
    "    'Ō': '',\n",
    "    'Ǭ': '',\n",
    "    'ł': '',\n",
    "    'þ': '',\n",
    "    'В': '',\n",
    "    'Κ': '',\n",
    "    'Τ': '',\n",
    "    'ν': '',\n",
    "    'ο': '',\n",
    "    'К': '',\n",
    "    'Р': '',\n",
    "    '·': '',\n",
    "    '·': '',\n",
    "    'Β': '',\n",
    "    'ё': '',\n",
    "    'У': '',\n",
    "    'Ε': '',\n",
    "    'Đ': '',\n",
    "    'đ': '',\n",
    "    'υ': '',\n",
    "    'Η': '',\n",
    "    'ı': '',\n",
    "    'О': '',\n",
    "    'І': '',\n",
    "    'ţ': '',\n",
    "    'о': '',\n",
    "    'Α': '',\n",
    "    'Μ': '',\n",
    "    'ν': '',\n",
    "    'Τ': '',\n",
    "    'ί': '',\n",
    "    'Á': '',\n",
    "    'á': '',\n",
    "    'Ć': '',\n",
    "    'ć': '',\n",
    "    'É': '',\n",
    "    'é': '',\n",
    "    'Í': '',\n",
    "    'í': '',\n",
    "    'Ń': '',\n",
    "    'ń': '',\n",
    "    'Ó': '',\n",
    "    'ó': '',\n",
    "    'Ś': '',\n",
    "    'ś': '',\n",
    "    'Ú': '',\n",
    "    'ú': '',\n",
    "    'Ý': '',\n",
    "    'ý': '',\n",
    "    'Â': '',\n",
    "    'â': '',\n",
    "    'Ê': '',\n",
    "    'ê': '',\n",
    "    'Î': '',\n",
    "    'î': '',\n",
    "    'Ô': '',\n",
    "    'ô': '',\n",
    "    'Û': '',\n",
    "    'û': '',\n",
    "    'Č': '',\n",
    "    'č': '',\n",
    "    'À': '',\n",
    "    'à': '',\n",
    "    'È': '',\n",
    "    'è': '',\n",
    "    'Ò': '',\n",
    "    'ò': '',\n",
    "    'Ù': '',\n",
    "    'ù': '',\n",
    "    'ñ': '',\n",
    "    'ã': '',\n",
    "    'ç': '',\n",
    "    'ū': '',\n",
    "    'ā': '',\n",
    "    'č': '',\n",
    "    'ě': '',\n",
    "    'ź': '',\n",
    "    'ć': '',\n",
    "    'ė': '',\n",
    "    'ŏ': '',\n",
    "    'ž': '',\n",
    "    'ő': '',\n",
    "    'ň': '',\n",
    "    '¯': '',\n",
    "    'ă': '',\n",
    "    'Č': '',\n",
    "    'š': '',\n",
    "    'ż': '',\n",
    "    'ř': '',\n",
    "    'ť': '',\n",
    "    'Š': '',\n",
    "    'ĕ': '',\n",
    "    'š': '',\n",
    "    'Č': '',\n",
    "    'ć': '',\n",
    "    'ĕ': '',\n",
    "    'ĭ': '',\n",
    "    'ș': '',\n",
    "    'ό': '',\n",
    "    'ģ': '',\n",
    "    'ț': '',\n",
    "    'ń': '',\n",
    "    'Ĭ': '',\n",
    "    'í': '',\n",
    "    'é': '',\n",
    "    'ô': '',\n",
    "    'è': '',\n",
    "    'ç': '',\n",
    "    'õ': '',\n",
    "    'ã': '',\n",
    "    'à': '',\n",
    "    'ú': '',\n",
    "    'á': '',\n",
    "    'ą': '',\n",
    "    'ə': '',\n",
    "    'î': '',\n",
    "    'í': '',\n",
    "    'ś': '',\n",
    "    'ō': '',\n",
    "    'Ż': '',\n",
    "    'â': '',\n",
    "    'Ŷ': '',\n",
    "    'Χ': '',\n",
    "    'х': '',\n",
    "    'е': '',\n",
    "    'р': '',\n",
    "    'А': '',\n",
    "    'Ǽ': '',\n",
    "    'Ǻ': '',\n",
    "    'ů': '',\n",
    "    '': '',\n",
    "    'Æ': '',\n",
    "    'æ': '',\n",
    "    'ӕ': '',\n",
    "    'ğ': '',\n",
    "    '': '',\n",
    "    'Ο': ''\n",
    "}\n",
    "\n",
    "def character_code_cleanup2(column):\n",
    "    for code, character in fixdict2.items():\n",
    "        column = [re.sub(code, character, entry) for entry in column]\n",
    "    return column\n",
    "\n",
    "def clean_review_in_set(dataframe, review_col):\n",
    "    reviews_list = dataframe[review_col].tolist()\n",
    "    reviews_list_cleaned = character_code_cleanup2(reviews_list)\n",
    "    dataframe = dataframe.drop(columns=[review_col])\n",
    "    dataframe[review_col] = reviews_list_cleaned\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f00608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function that can be used to mask sentences before passing them to BERT\n",
    "#this function requires review text and the indices of the sentences to be masked, and returns the masked text\n",
    "#this function will remove the intro just as the rule-based system does (the value should be the same as when you found the indices of the weightiest sentences), but add it back on again after the masking\n",
    "#also note that if the review is shorter than the number of sentences to mask, it will be empty, so you may need to manually replace the least weighty sentence afterwards\n",
    "\n",
    "def mask_sents(review, sents_to_mask, intro_to_ignore):\n",
    "    indexed_review = nltk.sent_tokenize(review)\n",
    "    num_sents = len(indexed_review)\n",
    "    intro_sents = math.floor(num_sents/(100/intro_to_ignore))\n",
    "    intro_bit = indexed_review[:intro_sents]\n",
    "    indexed_review = indexed_review[intro_sents:]\n",
    "    masked_review = []\n",
    "    if len(intro_bit) > 0:\n",
    "        for i in intro_bit:\n",
    "            masked_review.append(i)\n",
    "    for i, x in enumerate(indexed_review):\n",
    "        if i not in sents_to_mask:\n",
    "            masked_review.append(x)\n",
    "    finished_review = ' '.join([str(item) for item in masked_review])\n",
    "    return finished_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af69a2",
   "metadata": {},
   "source": [
    "## Results analysis\n",
    "This section contains functions for adding categories to the outputs, getting the best and worst results, and plotting sentence sentiment. This can also be done in conjunction with the earlier functions for checking metrics used during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c3c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function for adding categories to the results\n",
    "#this requires a dataframe containing results, including an 'Actual_rating' column and a 'Prediction' column\n",
    "#it counts a result above 5.0 as positive and below 5.1 as negative, but these may be adjusted as you wish, and further categories could easily be added by branching the if statements further\n",
    "\n",
    "def add_cats(resultsframe):\n",
    "    actual_cats = []\n",
    "    pred_cats = []\n",
    "    for rating in resultsframe['Actual_rating']:\n",
    "        if rating < 5.1:\n",
    "            actual_cats.append('Negative')\n",
    "        else:\n",
    "            actual_cats.append('Positive')\n",
    "    for prediction in resultsframe['Prediction']:\n",
    "        if rating < 5.1:\n",
    "            pred_cat.append('Negative')\n",
    "        else:\n",
    "            pred_cats.append('Positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44652b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function that can be used to get the best and worst results based on the difference between predictons and actual ratings\n",
    "#takes a dataframe of results as input, contained a column for \"Actual_rating\" and a column for \"Prediction\"\n",
    "#note that num_best and num_worst must be NEGATIVE versions of the desired numbers (i.e. if you want the top ten, then num_best = -10)\n",
    "\n",
    "def best_and_worst(resultsframe, num_best, num_worst):\n",
    "    actual_ratings_list = resultsframe['Actual_rating'].tolist()\n",
    "    predictions_list = resultsframe['Prediction'].tolist()\n",
    "    differences_list = []\n",
    "    for i, x in enumerate(actual_ratings_list):\n",
    "        differences_list.append(round(abs(x - predictions_list[i]), 1))\n",
    "    worst = sorted(range(len(differences_list)), key=lambda i: differences_list[i])[num_worst:]\n",
    "    best = sorted(range(len(differences_list)), key=lambda i: differences_list[i], reverse=True)[num_best:]\n",
    "    worst_results = resultsframe.iloc[worst]\n",
    "    best_results = resultsframe.iloc[best]\n",
    "    worst_results.to_csv('worstresults.csv', index=False)\n",
    "    best_results.to_csv('bestresults.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d3c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function that plots the sentiment scores of all sentences across a review\n",
    "#note, this function takes as input the output csv of score_aspects (mode 3). This file must be named 'review[ID].csv'\n",
    "#this function will display the plot and download it as a png\n",
    "\n",
    "def plot_sents(reviewid):\n",
    "    filetitle = 'review' + reviewid + '.csv'\n",
    "    results_sheet = pd.read_csv(filetitle, skiprows=1, header=None, names=['index', 'guitars', 'vocals', 'drums', 'production', 'breakdowns', 'lyrics', 'ambience', 'writing', 'bass', 'melodies', 'technicality', 'keyboards', 'symphonics', 'exotic instruments', 'creativity', 'comedy', 'catchiness', 'implicit', 'unattached', 'SENTENCE TOTAL'])\n",
    "    results_sheet.drop(results_sheet.tail(1).index,inplace=True)\n",
    "    index_list = results_sheet['index'].tolist()\n",
    "    sentence_score_list = results_sheet['SENTENCE TOTAL'].tolist()\n",
    "    plt.plot(index_list, sentence_score_list)\n",
    "    plt.xlabel('Sentence index')\n",
    "    plt.ylabel('Sentiment score')\n",
    "    plt.title(i)\n",
    "    savename = reviewid + '.png'\n",
    "    plt.savefig(savename, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
